---
title: Linear Regression
author: Jeremy Buss
date: '2022-01-03'
slug: []
categories: 
  - Machine Learning
  - Introduction to Statistical Learning - James/Witten/Hastie/Tibshirani
tags:
  - R
draft: yes
katex: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<ul>
<li>Interaction: two or more variables interact to affect a third variable in a non additive manner, i.e.Â two variables interact to have an affect more than the sum of their parts</li>
<li>Simple Linear Regression (formula): <span class="math inline">\(Y = \beta_0 + \beta_1 X + \epsilon\)</span></li>
<li>2 Coefficients or parameters for Simple Linear Regression (SLR):
<ul>
<li><span class="math inline">\(\beta_0=\)</span> Intercept</li>
<li><span class="math inline">\(\beta_1=\)</span> slope</li>
</ul></li>
<li>SLR estimate (formula): <span class="math inline">\(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x_i\)</span></li>
<li>Residual: difference between the <span class="math inline">\(i\)</span>th observed response and the <span class="math inline">\(i\)</span>th response value predicted by our model</li>
<li>Residual (formula): <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span></li>
<li>Residual Sum of Squares: the sum of squared residuals for all observations <span class="math inline">\(i=1, 2, \dots, n\)</span> a.k.a. <span class="math inline">\(RSS\)</span></li>
<li>Residual Sum of Squares (formula):
<ul>
<li><span class="math inline">\(RSS = e_1^2 + e_2^2 + \dots + e_n^2\)</span></li>
<li>$RSS = (y_1 - _0 - _1 x_1)^2 + (y_2 - _0 - _1 x_2)^2 + + (y_n - _0 - _1 x_n)^2 $</li>
</ul></li>
<li>Least Squares: an approach to estimate or choose values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by minimizing the <span class="math inline">\(RSS\)</span></li>
<li></li>
</ul>
