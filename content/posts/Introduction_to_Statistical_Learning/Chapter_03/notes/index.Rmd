---
title: Linear Regression
author: Jeremy Buss
date: '2022-01-03'
slug: []
categories: 
  - Machine Learning
  - Introduction to Statistical Learning - James/Witten/Hastie/Tibshirani
tags:
  - R
draft: yes
katex: yes
---

- Interaction: two or more variables interact to affect a third variable in a non additive manner, i.e. two variables interact to have an affect more than the sum of their parts
- Simple Linear Regression (formula): $Y = \beta_0 + \beta_1 X + \epsilon$
- 2 Coefficients or parameters for Simple Linear Regression (SLR): 
  - $\beta_0=$ Intercept
  - $\beta_1=$ slope 
- SLR estimate (formula): $\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x_i$
- Residual: difference between the $i$th observed response and the $i$th response value predicted by our model
- Residual (formula): $e_i = y_i - \hat{y}_i$
- Residual Sum of Squares: the sum of squared residuals for all observations $i=1, 2, \dots, n$ a.k.a. $RSS$
- Residual Sum of Squares (formula): 
  - $RSS = e_1^2 + e_2^2 + \dots + e_n^2$ 
  - $RSS = (y_1 - \hat{\beta}_0 - \hat{\beta}_1 x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1 x_2)^2 + \dots + (y_n - \hat{\beta}_0 - \hat{\beta}_1 x_n)^2 $
- Least Squares: an approach to estimate or choose values of $\hat{\beta}_0$ and $\hat{\beta}_1$ by minimizing the $RSS$
- Least Squares coefficient estimates:
  - $\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}$
  - $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$
- Expressions for sample means:
  - $\frac{1}{n}\sum_{i=1}^{n} {y_i}={\overline{y}}$
  - $\frac{1}{n}\sum_{i=1}^{n} {x_i}={\overline{x}}$
- Bias: Tendency of a statistic to over or under estimate a parameter
- Standard Error of a statistic: is the standard deviation of its sampling distribution
- Standard Error of the mean (formula): $\text{Var}(\hat{\mu})=\text{SE}(\hat{\mu})^2=\frac{\sigma^2}{n}$ or $\text{SE}(\hat{\mu})=\frac{\sigma}{\sqrt{n}}$; where $n$ = # of observations taken from statistical population and $\sigma$ is the standard deviation of the population
- Standard Deviation of a population: $\sigma$
- Standard Deviation of a sample: $\sigma_x$
- Standard Deviation of the mean (a.k.a. standard error of mean): $\sigma_{\bar{x}}=\frac{\sigma}{\sqrt{n}}$
- Estimator of Standard Deviation of the mean (colloqually called standard error): $\hat{\sigma}_{\bar{x}}=\frac{\sigma_x}{\sqrt{n}}$
- Standard Error of SLR Intercept: $\text{SE}(\hat{\beta}_0)^2=\sigma^2 \left [ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \right ]$; where $\sigma^2=\text{Var}(\epsilon)$ and the errors $\epsilon_i$ are uncorrelated
- Standard Error of SLR Slope: $\text{SE}(\hat{\beta}_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}$; where $\sigma^2=\text{Var}(\epsilon)$ and the errors $\epsilon_i$ are uncorrelated
- How do the values of $x$ affect the size of $\text{SE}(\hat{\beta}_1)$? The larger the spread of $x$ values (i.e. further from $\bar{x}$) the smaller the standard error of $\hat{\beta}_1$
- What happens if $\bar{x}=0$? Then $\text{SE}(\hat{\beta}_0)=\text{SE}(\hat{\mu})$ and $\hat{\beta}_0=\bar{y}$
- Residual Standard Error (formula): $\text{RSE}=\sqrt{\text{RSS}/n-2}$
- How should SE of $\beta_1$ be depicted if we are estimating $\sigma$? $\widehat{\text{SE}}(\hat{\beta}_1)$ to indicate that an estimate has been made; i.e. $\sigma \approx \text{RSE}$, but generally for simplicity of notation this extra "hat" is dropped
- Confidence Interval: Range of values such taht with a specific probability the range will contain the true unknown value of the parameter
- Approximate 95% CI of SLR coeff: 
  - $\hat{\beta}_1 \pm 2 \cdot  \text{SE}(\hat{\beta}_1)$
  - $\hat{\beta}_0 \pm 2 \cdot  \text{SE}(\hat{\beta}_0)$
- Hypothesis Test: a method of statistical inference used to determine possible conclusion from two different and likely conflicting hypothesis
- Null Hypothesis for SLR: There is no relationship between $X$ and $Y$ or $\hat{\beta}_1=0$
- Alternative Hypothesis for SLR: There is some relationship between $X$ and $Y$ or $\hat{\beta}_1 \ne 0$
- t-statistic: $t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}$
- t-distribution: any member of a family of continuous probability distributions that arise when estimating the mean of a normally distributed population in situations where the sample size is small and the population's standard deviation is unknown.
- Who developed the t-distribution? English statistician William Sealy Gosset under the pseudonym "Student"
- Why was the t-distribution first published under the pseudonym of "student"? Since William Sealy Gosset worked at Guinness at the time his employer didn't want competitors to know their steps
- p-value: probability of obtaining test results at least as extreme as the results observed, under assumption that the null hypothesis is correct
- Residual Standard Error: average amount response will deviate from true regression line; other words, if $\beta_0$ and $\beta_1$ were known perfectly; RSE is an estimate of standard deviation of $\epsilon$
- $R^2$ (formula): $\frac{TSS-RSS}{TSS}$ or $1-\frac{RSS}{TSS}$
- TSS (formula): $\sum_{i=1}^n(y_i-\bar{y})^2$
- Total Sum of Squares: TSS; Total variance in $Y_i$; amount of variability inherent in response prior to regression performed
- RSS: Amount of variability left unexplained after regression
- $R^2$: proportion of variability in $Y$ that can be explained by $X$; $R^2=r^2$ in SLR
- 