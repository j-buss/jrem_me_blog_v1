---
title: Introduction and Linear Algebra Review
author: Jeremy Buss
date: '2022-01-01'
slug: []
categories: 
  - Machine Learning
  - Introduction to Statistical Learning - James/Witten/Hastie/Tibshirani
tags:
  - R
draft: no
katex: yes
---

## Notation and Simple Matrix Algebra

- n: number of observations
- $x_{ij}$: the $j$-th variable of the $i$th observation where $i=1,2,\dots,n$ and $j=1,2,\dots,p$
- p: number of variables
- $\mathbf{X}$: is a matrix of size $n \times p$ whose $(i,j)$th element is $x_{ij}$
- $\mathbf{X}=$ $\begin{pmatrix} x_{11}&x_{12}&\dots&x_{1p} \\x_{21}&x_{22}&\dots&x_{2p}\\\vdots&\vdots&\ddots&\vdots\\x_{n1}&x_{n2}&\dots&x_{np} \end{pmatrix}$
- Row vector of $\mathbf{X}$: $x_i = \begin{pmatrix} x_{i1}\\x_{i2}\\\vdots\\x_{ip} \end{pmatrix}$; 
  - *Note: even though it is a __row__ vector the default orientation is as a column
- Column vector of $\mathbf{X}$: $\mathbf{x}_j = \begin{pmatrix} x_{1j}\\x_{2j}\\\vdots\\x_{nj} \end{pmatrix}$
- With these row and column conventions the entire matrix $\mathbf{X}$ can be written as:
  - Using Column Vectors: $\mathbf{X}=\begin{pmatrix} \mathbf{x}_1 & \mathbf{x}_2 & \dots & \mathbf{x}_p  \end{pmatrix}$
  - Using Row Vectors: $\mathbf{X} = \begin{pmatrix} x^{\intercal}_{1}\\x^{\intercal}_{2}\\\vdots\\x^{\intercal}_{n} \end{pmatrix}$
- order: refers to the number of rows and columns of a matrix
- rank of matrix: the number of linearly independent rows/columns
- column rank: the number of linearly independent columns
- row rank: the number of linearly independent rows
- Matrix $\mathbf{A}$ is full rank: if $\text{rank}(\mathbf{A})=min(n,p)$
- full rank if $n<p$: implies full row rank $\text{rank}(\mathbf{A})=n$
- full rank if $n>p$: implies full column rank $\text{rank}(\mathbf{A})=p$

- $\intercal$: transpose of a matrix
- $\mathbf{X}^{\intercal}=$ $\begin{pmatrix} x_{11}&x_{21}&\dots&x_{n1} \\x_{12}&x_{22}&\dots&x_{n2}\\\vdots&\vdots&\ddots&\vdots\\x_{1p}&x_{2p}&\dots&x_{np} \end{pmatrix}$
- $x_i^{\intercal} = \begin{pmatrix} x_{i1}&x_{i2}&\dots&x_{ip} \end{pmatrix}$
- $(\mathbf{A}^\intercal)^\intercal=\mathbf{A}$
- $(\mathbf{A}+\mathbf{B})^\intercal=\mathbf{A}^\intercal+\mathbf{B}^\intercal$
- $(b\mathbf{A})^\intercal=b\mathbf{A}^\intercal$ where $b$ is a scalar
- $(\mathbf{A}\mathbf{B})^\intercal=\mathbf{B}^\intercal\mathbf{A}^\intercal$
- $(\mathbf{A}^{-1})^\intercal=(\mathbf{A}^\intercal)^{-1}$
- trace of a square matrix $\mathbf{A}$ is the sum of the diagonals or $\text{tr}(\mathbf{A})=\sum_{j=1}^p a_{jj}$