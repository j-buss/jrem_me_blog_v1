---
title: Polynomial Regression
author: Jeremy Buss
date: '2021-11-14'
slug: []
categories:
  - Applied Stats
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Regression Analysis
tags:
  - linear regression
draft: False
katex: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="why-polynomial-regression" class="section level2">
<h2>Why Polynomial Regression</h2>
<ul>
<li>Polynomials are widely used in situations where the response surface is curvilinear</li>
<li>Many complex nonlinear relationships can be adequately modeled by polynomials over reasonably small ranges of the <span class="math inline">\(x\)</span>’s</li>
</ul>
<div id="polynomial-regression-models" class="section level3">
<h3>Polynomial Regression Models</h3>
<ul>
<li><p>A <strong>second</strong>-order polynomial in <strong>one</strong> variable or a <strong>quadratic</strong> model is
<span class="math display">\[
y=\beta_0 + \beta_1 x + \beta_2 x^2 + \epsilon
\]</span></p></li>
<li><p>A <strong>second</strong>-order polynomial in <strong>two</strong> variables is
<span class="math display">\[
y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{11}x_1^2 + \beta_{22}x_2^2 + \beta_{12} x_1 x_2 + \epsilon
\]</span></p></li>
<li><p>In general, the <span class="math inline">\(k\)</span><strong>th-order</strong> polynomail model in <strong>one</strong> variable is
<span class="math display">\[
y=\beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_{k}x^k + \epsilon
\]</span></p></li>
<li><p>If we set <span class="math inline">\(x_j=x^j\)</span>, this is just a multiple linear regression model with <span class="math inline">\(k\)</span> predictors <span class="math inline">\(x_1, x_2, \dots, x_k\)</span></p></li>
</ul>
</div>
<div id="important-considerations" class="section level3">
<h3>Important Considerations</h3>
<div id="keep-the-order-of-the-model-as-low-as-possible" class="section level4">
<h4>Keep the order of the model as low as possible</h4>
<ul>
<li>Try transformation to keep the model first order</li>
<li>If fails, try a 2nd order model</li>
<li>Avoid higher-order polynomials(<span class="math inline">\(k &gt; 2\)</span>) unless they can be justified for reasons outside the data</li>
<li><strong>Occam’s Razor</strong>: among competing models that predict equally well, shoose the “simplest” one, i.e., a <strong>parsimonious</strong> model:
<ul>
<li>Avoid <strong>overfitting</strong> that leads to nearly perfect fit to the data, but bad prediction performance</li>
</ul></li>
</ul>
</div>
<div id="model-building-strategy" class="section level4">
<h4>Model Building Strategy</h4>
<ul>
<li><strong>Forward Selection</strong>: successively fit models of increasing order until the <span class="math inline">\(t\)</span>-test for the highest order term is non-significant</li>
<li><strong>Backward Elimination</strong>: fit the highest order model and then delete terms one at a time until the highest order remaining term has a significant <span class="math inline">\(t\)</span> statistic</li>
<li>They do not necessarily lead to the same model</li>
</ul>
</div>
<div id="extrapolation" class="section level4">
<h4>Extrapolation</h4>
<ul>
<li>Can be extremely dangerous when the model is higher-order polynomial</li>
<li>The nature of the true underlying relationship may change or be completely different from the system that produced the data used to fit the model</li>
</ul>
</div>
<div id="ill-conditioning" class="section level4">
<h4>Ill-conditioning</h4>
<ul>
<li><strong>Ill-conditioning</strong>: <em>as the order of the model increases</em>, <span class="math inline">\(\mathbf{X}^{\intercal}\mathbf{X}\)</span> <em>matrix inversion will become inaccurate</em>, and error may be introduced into the parameter estimates</li>
<li>Centering the predictions may remove some ill conditioning but not all</li>
<li>One solution is to use <strong>orthogonal polynomials</strong></li>
</ul>
</div>
</div>
</div>
<div id="piecewise-polynomial-regression" class="section level2">
<h2>Piecewise (Polynomial) Regression</h2>
<ul>
<li><p>We may find that a polynomial regression provides a poor fit, and increasing the order does not improve the situation</p></li>
<li><p>This may happen when <em>the regression function behaves differently in different parts of the range of <span class="math inline">\(x\)</span></em></p></li>
<li><p><strong>Solution</strong>: <em>piecewise</em> polynomial regression that <strong>fits separate polynomials over different regions of <span class="math inline">\(x\)</span></strong></p></li>
<li><p>The joint points of pieces are call <strong>knots</strong></p></li>
<li><p>Example:
<span class="math display">\[
y= 
\begin{cases}
 \beta_{01} + \beta_{11} x + \beta_{21} x^2 + \beta_{31} x^3 + \epsilon &amp;\text{if } x &lt; c \\
 \beta_{02} + \beta_{12} x + \beta_{22} x^2 + \beta_{32} x^3 + \epsilon &amp;\text{if } x \ge c
\end{cases}
\]</span></p></li>
<li><p>Using more knots leads to a more flexible piecewise polynomial</p></li>
<li><p>With <span class="math inline">\(K\)</span> different knots, we fit <span class="math inline">\(K+1\)</span> different polynomials</p></li>
<li><p>Piecewise polynomials of order <span class="math inline">\(k\)</span> are called <strong>splines</strong>; (e.g. order 3 is a <strong>cubic spline</strong>)</p></li>
<li><p>When fitting piecewise polynomial regression the function may be discontinuous at the knote</p>
<ul>
<li>We could add additional constraints: that it must be continuous or <em>1st and 2nd derivatives</em> are continous at the knot</li>
</ul></li>
</ul>
<div id="splines-in-summary" class="section level3">
<h3>Splines in Summary</h3>
<ul>
<li>Fewer continuity restrictions:
<ul>
<li>more parameters in the model
_ better fit, but very easy to overfit</li>
</ul></li>
<li>More continuity restrictions:
<ul>
<li>the fit is worse</li>
<li>smoother regression curve</li>
</ul></li>
<li>Disadvanges: ill-conditioned <span class="math inline">\(\mathbf{X}^{\intercal}\mathbf{X}\)</span> if there are a lot of knots</li>
</ul>
</div>
</div>
