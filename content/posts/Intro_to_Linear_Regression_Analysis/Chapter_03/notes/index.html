---
title: Multiple Linear Regression
author: Jeremy Buss
date: '2021-10-09'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: true
katex: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="multiple-linear-regression-mlr" class="section level2">
<h2>Multiple Linear Regression (MLR)</h2>
<p>We have <span class="math inline">\(k\)</span> distinct predictors:
<span class="math display">\[
\begin{align}
Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \epsilon_i
\end{align}
\]</span></p>
<div id="sample-mlr-model" class="section level3">
<h3>Sample MLR Model</h3>
<p>Given data <span class="math inline">\((x_{11},\dots,x_{1k},y_1),(x_{21},\dots,x_{2k},y_2),\dots,(x_{n1},\dots,x_{nk},y_n)\)</span></p>
<p><span class="math display">\[
\begin{align*}
y_i &amp;= \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik} + \epsilon_i\\
&amp;= \beta_0 + \sum_{j=1}^k\beta_j x_{ij} + \epsilon_i,\quad i=1,2,\dots,n\\
\end{align*}
\]</span></p>
<p>The fit to this equation will be a hyperplane rather than a line as in SLR.</p>
<p>A model with <strong>interaction effects</strong> may result if there is a term <span class="math inline">\(x_1x_2\)</span>. It is still linear in <span class="math inline">\(\beta_{12}\)</span> and we could rewrite it as <span class="math inline">\(x_3 = x_1x_2\)</span> and let <span class="math inline">\(\beta_3 = \beta_{12}\)</span>. Additionally <strong>second order model interactions</strong> like <span class="math inline">\(...+\beta_{11}x_1^2+...\)</span> could be rewritten as <span class="math inline">\(x_3=x_1^2\)</span> and <span class="math inline">\(\beta_3 = \beta_{11}\)</span></p>
<p>In summary:</p>
<blockquote>
<p>Any regression model that is linear in the parameters (i.e. the <span class="math inline">\(\beta\)</span>’s) is a linear regression model, regardless of the shape of the surface that it generates
:Montgomer, Peck, Vinning pg. 69</p>
</blockquote>
</div>
<div id="point-estimation-of-model-parameters" class="section level3">
<h3>Point Estimation of Model Parameters</h3>
<p>The Least Squares function is</p>
<p><span class="math display">\[
S(\beta_0, \beta_1,\dots,\beta_k) = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left ( y_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij} \right )^2
\]</span></p>
<p>We minimize <span class="math inline">\(S\)</span> with respect to the coefficients <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_k\)</span></p>
<p><span class="math display">\[ 
\begin{align}
\frac{\partial S}{\partial {\beta_0}} \bigg\vert_{b_0,b_1,\dots,b_k} &amp;= -2\sum_{i=1}^n \left ( y_i - b_0 - \sum_{j=1}^k b_j x_{ij} \right ) = 0\\
\frac{\partial S}{\partial {\beta_j}} \bigg\vert_{b_0,b_1,\dots,b_k} &amp;= -2\sum_{i=1}^n \left ( y_i-b_0-\sum_{j=1}^k b_j x_{ij} \right ) x_{ij} = 0
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(p=k+1\)</span> equations with <span class="math inline">\(p\)</span> unknown parameters</li>
<li>The ordinary least squares estimators are the solutions to the normal equations.</li>
</ul>
</div>
<div id="regression-model-in-matrix-form" class="section level3">
<h3>Regression Model in Matrix Form</h3>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}
\]</span></p>
<p>where
<span class="math display">\[
\mathbf{y} = 
\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{bmatrix},
\mathbf{X} = 
\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \dots &amp; x_{1k}\\
1 &amp; x_{21} &amp; x_{22} &amp; \dots &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1 &amp; x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{nk}\\
\end{bmatrix},
\mathbf{\beta} = 
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_k
\end{bmatrix},
\mathbf{\epsilon} = 
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{bmatrix}
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span>: Design matrix</li>
<li><span class="math inline">\(\mathbf{\epsilon} \sim N(\mathbf{0},\sigma^2 \mathbf{I})\)</span></li>
</ul>
</div>
<div id="least-squares-estimation-in-matrix-form" class="section level3">
<h3>Least Squares Estimation in Matrix Form</h3>
<p><span class="math display">\[
\begin{align*}
S(\mathbf{\beta}) = \sum_{i=1}^n \epsilon_i^2 &amp;= \mathbf{\epsilon^\intercal} \mathbf{\epsilon} = (\mathbf{y} - \mathbf{X}\mathbf{\beta})^\intercal(\mathbf{y} - \mathbf{X}\mathbf{\beta})\\
&amp;=\mathbf{y}^\intercal\mathbf{y}-
\mathbf{\beta^\intercal}\mathbf{X}^\intercal\mathbf{y}-
\mathbf{y^\intercal}\mathbf{X}\mathbf{\beta}+
\mathbf{\beta^\intercal}\mathbf{X^\intercal}\mathbf{X}\mathbf{\beta}
\end{align*}
\]</span>
Since <span class="math inline">\(\mathbf{\beta^\intercal}\mathbf{X}^\intercal\mathbf{y}\)</span> is a <span class="math inline">\(1\times1\)</span> matrix and it’s transpose <span class="math inline">\(\mathbf{y^\intercal}\mathbf{X}\mathbf{\beta}\)</span> is the same scalar we will combine the two terms and we are left with:</p>
<p><span class="math display">\[
\begin{align}
S(\mathbf{\beta}) &amp;=\mathbf{y}^\intercal\mathbf{y}-
2\mathbf{\beta^\intercal}\mathbf{X}^\intercal\mathbf{y}+
\mathbf{\beta^\intercal}\mathbf{X^\intercal}\mathbf{X}\mathbf{\beta}
\end{align}
\]</span></p>
<p>Before we go further we need to know a few calculus matrix rules:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\frac{\partial{\mathbf{t}^\intercal} \mathbf{a}}{\partial{t}} = \frac{\partial{\mathbf{a}^\intercal} \mathbf{t}}{\partial{t}} = \mathbf{a}\)</span></li>
<li><span class="math inline">\(\frac{\partial{\mathbf{t}^\intercal} \mathbf{A} \mathbf{t}}{\partial{t}} = 2 \mathbf{A}\mathbf{t}\)</span></li>
</ol>
<p>With regards to equation <span class="math inline">\(\textcolor{red}{(4)}\)</span> we take each of the terms in order:</p>
<ul>
<li>The <span class="math inline">\(\mathbf{y}^\intercal\mathbf{y}\)</span> term does not have a <span class="math inline">\(\beta\)</span> term and therefore the derivative is 0</li>
<li>With the second term: <span class="math inline">\(-2\mathbf{\beta^\intercal}\mathbf{X}^\intercal\mathbf{y}\)</span> we will use the first of the derivative rules and simply end up with <span class="math inline">\(-2\mathbf{X}^\intercal\mathbf{y}\)</span></li>
<li>With the third term <span class="math inline">\(\mathbf{\beta^\intercal}\mathbf{X^\intercal}\mathbf{X}\mathbf{\beta}\)</span> we use the 2nd matrix derivative rule. So see this we let <span class="math inline">\(\beta\)</span> equal to the <span class="math inline">\(t\)</span> in the rule and <span class="math inline">\(\mathbf{X}^\intercal\mathbf{X}\)</span> be equal to the <span class="math inline">\(\mathbf{A}\)</span> and substituting in <span class="math inline">\(\mathbf{b}\)</span> for <span class="math inline">\(\mathbf{\beta}\)</span></li>
</ul>
<p>So together we are left with:</p>
<div id="normal-equation" class="section level4">
<h4>Normal Equation</h4>
<p><span class="math display">\[
\frac{\partial{S}}{\partial\beta}\bigg\vert_{\mathbf{b}} = -2 \mathbf{X^\intercal} \mathbf{y} + 2 \mathbf{X^\intercal} \mathbf{X} \mathbf{b} = 0
\]</span>
and then solving for <span class="math inline">\(\mathbf{b}\)</span> we have:</p>
<p><span class="math display">\[
\begin{align}
\mathbf{b} = ( \mathbf{X}^\intercal \mathbf{X}) ^ {-1} \mathbf{X}^\intercal \mathbf{y}
\end{align}
\]</span></p>
</div>
<div id="hat-matrix" class="section level4">
<h4>Hat Matrix</h4>
<p>The vector of fitted values <span class="math inline">\(\hat{y_i}\)</span> corresponding to <span class="math inline">\(y_i\)</span> is:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{X} \mathbf{b} = \mathbf{X} ( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal} \mathbf{y} = \mathbf{H} \mathbf{y}
\]</span></p>
<p>Whereby the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{X} ( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal}\)</span> is called the <strong>hat matrix</strong>.</p>
<p>The vector of residuals <span class="math inline">\(e_i = y_i = \hat{y_i}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{e} = \mathbf{y}-\mathbf{\hat{y}} = \mathbf{y} - \mathbf{Xb} = \mathbf{y}-\mathbf{Hy}=(\mathbf{I}-\mathbf{H})\mathbf{y}
\]</span></p>
<p>Note:</p>
<ul>
<li>Both <span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{I-H}\)</span> are <strong>symmetric</strong> and <strong>idempotent</strong>. In other words, they are <strong>projection</strong> matrices</li>
<li><span class="math inline">\(\mathbf{H}\)</span> projects <span class="math inline">\(\mathbf{y}\)</span> to <span class="math inline">\(\mathbf{\hat{y}}\)</span> on the <span class="math inline">\(p\)</span>-dimension space spanned by columns of <span class="math inline">\(\mathbf{X}\)</span>, or the column space of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(Col(\mathbf{X})\)</span></li>
<li><span class="math inline">\(\mathbf{I-H}\)</span> projects <span class="math inline">\(\mathbf{y}\)</span> to <span class="math inline">\(\mathbf{e}\)</span> on the space <strong>perpendicular</strong> to <span class="math inline">\(Col(\mathbf{x})\)</span>, or <span class="math inline">\(Col(X)^\bot\)</span></li>
</ul>
<p>lkjhlkjh</p>
<p><img src="images/Geometricl_Interpretation_of_Least_Squares.png" />
lkjhlkjh</p>
</div>
</div>
</div>
