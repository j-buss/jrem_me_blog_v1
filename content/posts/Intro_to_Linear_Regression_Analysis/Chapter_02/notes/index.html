---
title: Intro. to Linear Regression Analysis - Chapter 2 - Simple Linear Regression
author: Jeremy Buss
date: '2021-09-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="simple-linear-regression" class="section level1">
<h1>Simple Linear Regression</h1>
<p>The characteristics of a <strong>Simple Linear Regression</strong> are as follows:</p>
<ul>
<li>One predictor <span class="math inline">\(X\)</span> that is known and constant</li>
<li>One response variable <span class="math inline">\(Y\)</span></li>
<li>Linear function <span class="math inline">\(y=\beta_0 + \beta_1 x\)</span></li>
</ul>
<p><br></p>
<div id="simple-linear-regression---population" class="section level2">
<h2>Simple Linear Regression - Population</h2>
<p>The above is an idealized representation. Instead we have a set of n-pairs of data <span class="math inline">\((x_i, y_i)\)</span> where <span class="math inline">\(i=1, 2, ... n\)</span>. Additionally, the data will not perfectly fit the line. As a result we add an error term <span class="math inline">\(\epsilon\)</span>.</p>
<ul>
<li><p>Linear function <span class="math inline">\(Y_i=\beta_0 + \beta_1 X_i + \epsilon_i\)</span> for <span class="math inline">\(i=1,2,...n\)</span></p></li>
<li><p>Error term <span class="math inline">\(\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)\)</span></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"><span class="math inline">\(iid\)</span></a>: identically, and independently distributed</li>
<li><span class="math inline">\(E(\epsilon_i)=0\)</span></li>
<li><span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span></li>
</ul></li>
<li><p><span class="math inline">\(Y_i\)</span> is the <span class="math inline">\(i\)</span>-th value of the response</p></li>
<li><p><span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>-th value of the fixed value of the predictor</p></li>
<li><p><span class="math inline">\(\beta_0, \beta_1,\)</span> are model coefficients</p></li>
<li><p><span class="math inline">\(\beta_0, \beta_1,\)</span> and <span class="math inline">\(\sigma^2\)</span> are fixed unknown parameters of the model</p></li>
<li><p>The mean response <span class="math inline">\(\mu_{Y|X}=E(Y|X)\)</span> has a straightline relationship with <span class="math inline">\(X\)</span> given by the population regression line
<span class="math display">\[
\begin{align*}
\mu_{Y_i|X_i}=E(Y_i|X_i)&amp;=E(\beta_0+\beta_1 X_i + \epsilon_i)\\
&amp;=\beta_0+\beta_1 X_i + E(\epsilon_i)\\
&amp;=\beta_0+\beta_1 X_i
\end{align*}
\]</span></p></li>
<li><p>The variance of <span class="math inline">\(Y\)</span> does not depend on <span class="math inline">\(X\)</span>
<span class="math display">\[
\begin{align*}
Var(Y_i|X_i) &amp;= Var(\beta_0+\beta_1 X_i + \epsilon_i)\\
&amp;=Var(\epsilon_i)
\end{align*}
\]</span></p></li>
<li><p>Any fixed value of <span class="math inline">\(X_i=x\)</span>, the response variable <span class="math inline">\(Y_i\)</span> varies with <span class="math inline">\(N(\mu_{Y_i|X_i=x},\sigma^2)\)</span></p>
<ul>
<li><span class="math inline">\(Y_i|X_i = N(\beta_0 + \beta_1 X_i, \sigma^2)\)</span></li>
</ul></li>
</ul>
</div>
<div id="parameter-estimation-and-model-fitting" class="section level2">
<h2>Parameter Estimation and Model Fitting</h2>
<p>Given that we have sample data <span class="math inline">\(\{(x_1, y_1),(x_2, y_2),...(x_n,y_n)\}\)</span> we wish to find the <em>best</em> sample regression line.</p>
<p>We are interested in the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the following <em>sample</em> regression model:
<span class="math display">\[
\begin{align*}
y_i &amp;= f(x_i) + \epsilon_i\\
&amp;= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}
\]</span></p>
<p>or using sample statistics <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> computed from the training data to estimate them:
<span class="math display">\[\hat{y_i} = b_0 + b_1 x_i\]</span>
where <span class="math inline">\(\hat{y_i}\)</span> is a point estimate of <span class="math inline">\(y_i\)</span> with mean <span class="math inline">\(\mu_{Y|X=x_i}=E(Y|X=x_i)\)</span></p>
</div>
<div id="ordinary-least-squares-ols" class="section level2">
<h2>Ordinary Least Squares (OLS)</h2>
<p>We choose <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that minimize the <strong>sum of squared residuals</strong> <span class="math inline">\(SS_{res}\)</span> or <strong>residual sum of squares</strong> <span class="math inline">\(RSS\)</span></p>
<ul>
<li><strong>residual</strong> <span class="math inline">\(e_i\)</span> is a point estimate of <span class="math inline">\(\epsilon_i\)</span> and defined as the difference between our estimate of <span class="math inline">\(y_i\)</span> (i.e. <span class="math inline">\(\hat{y_i}\)</span>) and the true value of <span class="math inline">\(y_i\)</span>
<ul>
<li><span class="math inline">\(e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)\)</span></li>
</ul></li>
<li>The sample regression line minimizes <span class="math inline">\(SS_{res}\)</span>
<span class="math display">\[
\begin{align*}
SS_{res}&amp;=e_1^2 + e_2^2 + \cdots + e_n^2 = \sum_{i=1}^n e_i^2\\
&amp;=(y_1 - b_0 + b_1 x_1)^2 + (y_2 - b_0 + b_1 x_2)^2 + \cdots + (y_n - b_0 + b_1 x_n)^2\\
&amp;= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\\
\end{align*}
\]</span></li>
</ul>
</div>
<div id="least-squares-estimate-lse" class="section level2">
<h2>Least Squares Estimate (LSE)</h2>
<p>With the least squares estimate approach we choose <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> to minimize <span class="math inline">\(SS_{res}\)</span></p>
<p><span class="math display">\[
(b_0, b_1) = \text{arg} \; \underset {\beta_0, \beta_1}{\text{min}} \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2
\]</span>
Taking the derivative and set them equal to 0 to arrive at the <strong>normal equations</strong>:</p>
<p><span class="math display">\[ 
\begin{align}
\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} &amp;= -2\sum_{i=1}^n (y_i-b_0-b_1x_i)\\
\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} &amp;= -2\sum_{i=1}^n x_i(y_i-b_0-b_1x_i)
\end{align}
\]</span></p>
<div id="least-squares-estimates-solving-for-beta_0-and-beta_1" class="section level3">
<h3>Least Squares Estimates: Solving for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></h3>
<ul>
<li><p>Solving for <span class="math inline">\(\beta_0\)</span> given <span class="math inline">\(b_1\)</span>:
<span class="math display">\[b_0 = \overline{y} - b_1 \overline {x}\]</span></p></li>
<li><p>Solving for <span class="math inline">\(\beta_1\)</span> given that <span class="math inline">\(b_0 = \overline{y} - b_1 \overline {x}\)</span>:
<span class="math display">\[
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} = 
\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
=\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=
\frac{S_{xy}}{S_{xx}}
\end{align*}
\]</span></p></li>
</ul>
</div>
<div id="best-linear-unbiased-estimators-blue" class="section level3">
<h3>Best Linear Unbiased Estimators (BLUE)</h3>
<div id="best" class="section level4">
<h4>1. Best:</h4>
<p>For the estimators to be the “best” they need to have minimum variance.</p>
<div id="variance-of-beta_1" class="section level5">
<h5>Variance of <span class="math inline">\(\beta_1\)</span></h5>
<p><span class="math display">\[
\begin{align*}
Var(b_1)&amp;=Var \left (\sum_{i=1}^n c_i y_i \right ) = \sum_{i=1}^n c_i^2 Var(y_i)
\end{align*}
\]</span></p>
<p>Since the observations <span class="math inline">\(y_i\)</span> are uncorrelated then the variance of the sum is just the sum of the variances. Since the <span class="math inline">\(Var(y_i)=\sigma^2\)</span></p>
<p><span class="math display">\[
\begin{align*}
Var(b_1)&amp;= \sigma^2 \sum_{i=1}^n c_i^2 = \frac {\sigma^2 \sum_{i=1}^n (x_i-\overline {x})^2}{S_{xx}} = \frac{\sigma^2}{S_{xx}}
\end{align*}
\]</span></p>
</div>
<div id="the-variance-of-b_0" class="section level5">
<h5>The Variance of <span class="math inline">\(b_0\)</span></h5>
<p><span class="math display">\[
\begin{align*}
Var(b_0) &amp;= Var(\overline{y} - b_1 \overline {x})
\end{align*}
\]</span>
Using the <a href="https://en.wikipedia.org/wiki/Variance#Properties">properties of variance</a> for the difference of two random variables this becomes:
<span class="math display">\[
\begin{align}
Var(b_0) &amp;= Var(\overline{y}) + \overline{x}^2 Var (b_1) - 2 \overline{x} Cov(\overline{y},b_1)
\end{align}
\]</span>
To simplify this further we need to find <span class="math inline">\(Var(\overline{y})\)</span></p>
<p><span class="math display">\[
\begin{align*}
Var(\overline {y}) &amp;= Var \left ( \frac{y_1+y_2+\dots+y_n}{n} \right )\\
&amp;= \left ( \frac {1}{n} \right )^2 (y_1+y_2+\dots+y_n)\\
&amp;= \left ( \frac {1}{n} \right )^2 (\sigma^2+\sigma^2+\dots+\sigma^2)\\
&amp;= \left ( \frac {1}{n} \right )^2 n \sigma^2\\
&amp;= \frac {\sigma^2}{n} 
\end{align*}
\]</span>
Additionally we can simplify the last term in equation #3 and focus on the Covariance.</p>
<p>Let us start by simply filling in the values for <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(b_1\)</span>.</p>
<p><span class="math display">\[
Cov(\overline{y},b_1)=
Cov\left(\frac{1}{n}\sum_{i=1}^ny_i,    \frac {\sum_{i=1}^ny_i(x_i-\overline{x})} {S_{xx}} \right)
\]</span>
A constant can be pulled out of the covariance and simply multiplied.
<span class="math display">\[
Cov(\overline{y},b_1)=
\frac{1}{n}Cov\left(\sum_{i=1}^ny_i,    \frac {\sum_{i=1}^ny_i(x_i-\overline{x})} {S_{xx}} \right)
\]</span>
Next we let <span class="math inline">\(c_i = \frac{(x_i - \overline{x})}{S_{xx}}\)</span>:</p>
<p><span class="math display">\[
Cov(\overline{y},b_1)=
\frac{1}{n}Cov\left(\sum_{i=1}^ny_i,    \sum_{i=1}^nc_iy_i \right)
\]</span></p>
<p>Now since <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(b_1\)</span> are independently distributed normal variables we can pull the <span class="math inline">\(c_i\)</span> term out of the <span class="math inline">\(Cov\)</span></p>
<p><span class="math display">\[
\begin{split}
Cov(\overline{y},b_1) =
\frac{1}{n}\sum_{i=1}^nc_i \cdot Cov\left(\sum_{i=1}^ny_i,    \sum_{i=1}^ny_i \right)\\
\\
Cov(\overline{y},b_1)= \frac{1}{n}\sum_{i=1}^nc_i \cdot Var(y_i)\\
\\
Cov(\overline{y},b_1)= \frac{\sigma^2}{n}\sum_{i=1}^nc_i\\
\end{split}
\]</span>
Now if we remember that <span class="math inline">\(c_i = \frac{(x_i - \overline{x})}{S_{xx}}\)</span> and sum of which will be 0 so:
<span class="math display">\[
\begin{split}
Cov(\overline{y},b_1)&amp;= \frac{\sigma^2}{n}\sum_{i=1}^n \frac{(x_i - \overline{x})}{S_{xx}}\\
\\
Cov(\overline{y},b_1)&amp;= \frac{\sigma^2}{n} \cdot 0\\
\\
Cov(\overline{y},b_1)&amp;= 0\\
\end{split}
\]</span>
So we see that the last term in equation #3 is 0. So we are left with:</p>
<p><span class="math display">\[
\begin{align*}
Var(b_0) &amp;= Var(\overline{y}) + \overline{x}^2 Var (b_1)\\
&amp;= Var(\overline{y}) + \overline{x}^2 Var (b_1)\\
&amp;= \frac{\sigma^2}{n} + \frac{\overline{x}^2\sigma^2}{S_{xx}}\\
Var(b_0)&amp;= \sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \right )
\end{align*}
\]</span></p>
</div>
</div>
<div id="linear" class="section level4">
<h4>2. Linear:</h4>
<p><span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are linear combinations of <span class="math inline">\(y_i\)</span> for example for <span class="math inline">\(b_1\)</span> we use one of the forms:
<span class="math display">\[
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
\end{align*}
\]</span>
To clean up the equation we use the term <span class="math inline">\(c_i = \frac{x_i-\overline{x}}{S_{xx}}\)</span> for <span class="math inline">\(i=1,2,\dots n\)</span> and the equation becomes simply a constant multiplied by <span class="math inline">\(y_i\)</span>:
<span class="math display">\[
\begin{align*}
b_1 = \sum_{i=1}^ny_i c_i
\end{align*}
\]</span></p>
</div>
<div id="unbiased" class="section level4">
<h4>3. Unbiased</h4>
<p><span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></p>
<p>For <span class="math inline">\(b_0\)</span> to be an unbiased estimator we want to show that <span class="math inline">\(E(b_0)=\beta_0\)</span>. Let us simply start by substituting in the value for <span class="math inline">\(b_0\)</span> from the normal equations.</p>
<p><span class="math display">\[
E(b_0) = E(\overline{y} - b_1\overline{x})
\]</span>
We use the helper function <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n} {y_i}={\overline{y}}\)</span> and pull out any values from the sum or Expected value function that are not dependent on them
<span class="math display">\[
E(b_0) = \frac{1}{n}\sum_{i=1}^n E({y_i}) - E(b_1)\overline{x}
\]</span>
Now substitute our equation for <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i\)</span> and <span class="math inline">\(E(b_1)=\beta_1\)</span> in addition to the fact that <span class="math inline">\(\beta_0, \beta_1, x_i\)</span> are constant values the Expected Value of those individual items is simply the value of those items…
<span class="math display">\[
E(b_0) = \frac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 x_i) - \beta_1 \overline{x}
\]</span>
Again we use a helper function for <span class="math inline">\(\sum_{i=1}^{n} {x_i}=n{\overline{x}}\)</span>
<span class="math display">\[
E(b_0) = \frac{1}{n}(n\beta_0 + n\beta_1 \overline{x}) - \beta_1 \overline{x}
\]</span>
Combining like terms we are left with simply:
<span class="math display">\[
E(b_0) = \beta_0 
\]</span></p>
<p>Similarly for <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
E(b_1)&amp;=E \left (\sum_{i=1}^n c_i y_i \right ) = \sum_{i=1}^n c_i E(y_i)\\
&amp;=\sum_{i=1}^n c_i (b_0 + b_1 x_i)=b_0 \sum_{i=1}^n c_i + b_1 \sum_{i=1}^n c_i x_i
\end{align*}
\]</span></p>
<p>Now <span class="math inline">\(\sum_{i=1}^n c_i =0\)</span> and <span class="math inline">\(\sum_{i=1}^n c_i x_i =1\)</span> so
<span class="math display">\[
E(b_1)=\beta_1
\]</span></p>
</div>
<div id="properties-of-least-squares-fit" class="section level4">
<h4>Properties of Least Squares Fit</h4>
<ol style="list-style-type: decimal">
<li>Sum of residuals is zero <span class="math inline">\(\sum_{i=1}^n(y_i - \hat {y_i}) = \sum_{i=1}^n e_i = 0\)</span></li>
<li>The sum of observations <span class="math inline">\(y_i\)</span> and the sum of fitted values <span class="math inline">\(\hat{y_i}\)</span> are the same <span class="math inline">\(\sum_{i=1}^ny_i = \sum_{i=1}^n \hat{y}\)</span></li>
<li>The Least Squares Regression line passes through the centroid point <span class="math inline">\((\overline{x},\overline{y})\)</span></li>
<li>Inner product of residual and predictor is zero <span class="math inline">\(\sum_{i=1}^n e_i x_i = 0\)</span></li>
<li>Inner product of residual and predicted values is zero <span class="math inline">\(\sum_{i=1}^n e_i \hat{y_i} = 0\)</span></li>
</ol>
</div>
<div id="estimation-for-sigma2" class="section level4">
<h4>Estimation for <span class="math inline">\(\sigma^2\)</span></h4>
<p>The <strong>error</strong> or <strong>residual sum of squares</strong> is:
<span class="math display">\[SS_{res} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2\]</span></p>
<p>The residual sum of squares has <span class="math inline">\(n-2\)</span> degrees of freedom, because two degrees of freedom are associated with the estimators <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> involved in obtaining <span class="math inline">\(\hat{y_i}\)</span>. Dividing the residual sum of squares by the degrees of freedom we arrive at the <strong>residual mean square</strong></p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac {SS_{Res}}{n-2}=MS_{Res}\]</span>
<span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>, i.e. <span class="math inline">\(E(MS_{Res})=\sigma^2\)</span></p>
</div>
</div>
<div id="sampling-distribution" class="section level3">
<h3>Sampling Distribution</h3>
<p>We do not know the true values of <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span>. Therefore we must derive them from the data. As such we must take into account how these values are derived and what the likelihood of the true value being within a certain range.</p>
<div id="sampling-distribution-of-beta_0-and-beta_1" class="section level4">
<h4>Sampling Distribution of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></h4>
<p>We know that the errors <span class="math inline">\(\epsilon_i\)</span> are <span class="math inline">\(NID(0,\sigma^2)\)</span> and the observations <span class="math inline">\(y_i\)</span> are <span class="math inline">\(NID(\beta_0 + \beta_1 x_i, \sigma^2)\)</span>. Also, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are linear combinations of <span class="math inline">\(y_i\)</span>. Therefore <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are normally distributed. We can synthesize the information this way:</p>
<p><span class="math display">\[
\begin{align*}
b_1 &amp; \sim NID \left ( \beta_1,\frac{\sigma^2}{S_{xx}} \right )\\
b_0 &amp; \sim NID \left ( \beta_0,\sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}}\right ) \right )
\end{align*}
\]</span>
Since we just said that they are normal we could standardize them:</p>
<p><span class="math display">\[
\begin{align*}
\frac {b_1 - \beta_1}{\sqrt{\sigma^2/S_{xx}}} &amp; \sim NID (0,1)\\
\frac {b_0 - \beta_0}{\sqrt{ \sigma^2 (1/n + \overline{x}^2/S_{xx})}} &amp; \sim NID (0,1)\\
\end{align*}
\]</span>
However, we don’t know the true value of <span class="math inline">\(\sigma^2\)</span>. Therefore we can use the estimate of <span class="math inline">\(\sigma\)</span> that we found earlier: <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\frac {b_1 - \beta_1}{\sqrt{\hat{\sigma}^2/S_{xx}}} &amp; \sim t_{n-2}\\
\frac {b_0 - \beta_0}{\sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}} &amp; \sim t_{n-2}\\
\end{align}
\]</span>
We can use this information to construct a <span class="math inline">\((1-\alpha)100%\)</span> confidence intervals:</p>
<ul>
<li><span class="math inline">\(\beta_1\)</span>: <span class="math inline">\(b_1 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2/S_{xx}}\)</span></li>
<li><span class="math inline">\(\beta_0\)</span>: <span class="math inline">\(b_0 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}\)</span></li>
</ul>
</div>
<div id="sampling-distribution-of-hatsigma2" class="section level4">
<h4>Sampling Distribution of <span class="math inline">\(\hat{\sigma^2}\)</span></h4>
<p>It can be shown that <span class="math inline">\(\frac {SS_{res}}{\sigma^2}\)</span> follows a chi-squared distribution. We can use that fact to the sampling distribution of <span class="math inline">\(\hat{\sigma^2}\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
\frac {SS_{res}}{\sigma^2} &amp; \sim \chi_{n-2}^2\\
SS_{res} &amp; \sim \sigma^2 \chi_{n-2}^2\\
\frac {1}{n-2}SS_{res} &amp; \sim \frac {1}{n-2} \sigma^2 \chi_{n-2}^2\\
\hat{\sigma^2} = MS_{res} &amp; \sim \sigma^2 \frac {\chi_{n-2}^2}{n-2}  \\
\end{align*}
\]</span></p>
<p>We can use this information to construct a <span class="math inline">\((1-\alpha)100%\)</span> confidence interval:</p>
<ul>
<li><span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\left ( \frac{SS_{res}}{{\chi_{\alpha/2,n-2}^2}}, \frac{SS_{res}}{{\chi_{1-\alpha/2,n-2}^2}}\right )\)</span></li>
</ul>
</div>
</div>
<div id="hypothesis-testing" class="section level3">
<h3>Hypothesis Testing</h3>
<div id="standard-error" class="section level4">
<h4>Standard Error</h4>
<p>The denominator of the test statistics (equations #4 and #5) are often referred to as the <strong>standard error</strong>. Rewriting each of the terms as:</p>
<ul>
<li><span class="math inline">\(se(\hat{\beta_1}) = \sqrt{\frac{MS_{Res}}{S_{xx}}}\)</span></li>
<li><span class="math inline">\(se(\hat{\beta_0}) = \sqrt{MS_{Res} (1/n + \overline{x}^2/S_{xx})}\)</span></li>
</ul>
</div>
</div>
</div>
</div>
