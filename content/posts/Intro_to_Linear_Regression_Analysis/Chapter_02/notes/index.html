---
title: Intro. to Linear Regression Analysis - Chapter 2 - Simple Linear Regression
author: Jeremy Buss
date: '2021-09-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="simple-linear-regression" class="section level3">
<h3>Simple Linear Regression</h3>
<p>The characteristics of a <strong>Simple Linear Regression</strong> are as follows:</p>
<ul>
<li>One predictor <span class="math inline">\(X\)</span> that is known and constant</li>
<li>One response variable <span class="math inline">\(Y\)</span></li>
<li>Linear function <span class="math inline">\(y=\beta_0 + \beta_1 x\)</span></li>
</ul>
<p><br></p>
<div id="simple-linear-regression---population" class="section level4">
<h4>Simple Linear Regression - Population</h4>
<p>The above is an idealized representation. Instead we have a set of n-pairs of data <span class="math inline">\((x_i, y_i)\)</span> where <span class="math inline">\(i=1, 2, ... n\)</span>. Additionally, the data will not perfectly fit the line. As a result we add an error term <span class="math inline">\(\epsilon\)</span>.</p>
<ul>
<li><p>Linear function <span class="math inline">\(Y_i=\beta_0 + \beta_1 X_i + \epsilon_i\)</span> for <span class="math inline">\(i=1,2,...n\)</span></p></li>
<li><p>Error term <span class="math inline">\(\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)\)</span></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"><span class="math inline">\(iid\)</span></a>: identically, and independently distributed</li>
<li><span class="math inline">\(E(\epsilon_i)=0\)</span></li>
<li><span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span></li>
</ul></li>
<li><p><span class="math inline">\(Y_i\)</span> is the <span class="math inline">\(i\)</span>-th value of the response</p></li>
<li><p><span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>-th value of the fixed value of the predictor</p></li>
<li><p><span class="math inline">\(\beta_0, \beta_1,\)</span> are model coefficients</p></li>
<li><p><span class="math inline">\(\beta_0, \beta_1,\)</span> and <span class="math inline">\(\sigma^2\)</span> are fixed unknown parameters of the model</p></li>
<li><p>The mean response <span class="math inline">\(\mu_{Y|X}=E(Y|X)\)</span> has a straightline relationship with <span class="math inline">\(X\)</span> given by the population regression line
<span class="math display">\[
\begin{align*}
\mu_{Y_i|X_i}=E(Y_i|X_i)&amp;=E(\beta_0+\beta_1 X_i + \epsilon_i)\\
&amp;=\beta_0+\beta_1 X_i + E(\epsilon_i)\\
&amp;=\beta_0+\beta_1 X_i
\end{align*}
\]</span></p></li>
<li><p>The variance of <span class="math inline">\(Y\)</span> does not depend on <span class="math inline">\(X\)</span>
<span class="math display">\[
\begin{align*}
Var(Y_i|X_i) &amp;= Var(\beta_0+\beta_1 X_i + \epsilon_i)\\
&amp;=Var(\epsilon_i)
\end{align*}
\]</span></p></li>
<li><p>Any fixed value of <span class="math inline">\(X_i=x\)</span>, the response variable <span class="math inline">\(Y_i\)</span> varies with <span class="math inline">\(N(\mu_{Y_i|X_i=x},\sigma^2)\)</span></p>
<ul>
<li><span class="math inline">\(Y_i|X_i = N(\beta_0 + \beta_1 X_i, \sigma^2)\)</span></li>
</ul></li>
</ul>
</div>
</div>
<div id="parameter-estimation-and-model-fitting" class="section level3">
<h3>Parameter Estimation and Model Fitting</h3>
<p>Given that we have sample data <span class="math inline">\(\{(x_1, y_1),(x_2, y_2),...(x_n,y_n)\}\)</span> we wish to find the <em>best</em> sample regression line.</p>
<p>We are interested in the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the following <em>sample</em> regression model:
<span class="math display">\[
\begin{align*}
y_i &amp;= f(x_i) + \epsilon_i\\
&amp;= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}
\]</span></p>
<p>or using sample statistics <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> computed from the training data to estimate them:
<span class="math display">\[\hat{y_i} = b_0 + b_1 x_i\]</span>
where <span class="math inline">\(\hat{y_i}\)</span> is a point estimate of <span class="math inline">\(y_i\)</span> with mean <span class="math inline">\(\mu_{Y|X=x_i}=E(Y|X=x_i)\)</span></p>
<div id="ordinary-least-squares-ols" class="section level4">
<h4>Ordinary Least Squares (OLS)</h4>
<p>We choose <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that minimize the <strong>sum of squared residuals</strong> <span class="math inline">\(SS_{res}\)</span> or <strong>residual sum of squares</strong> <span class="math inline">\(RSS\)</span></p>
<ul>
<li><strong>residual</strong> <span class="math inline">\(e_i\)</span> is a point estimate of <span class="math inline">\(\epsilon_i\)</span> and defined as the difference between our estimate of <span class="math inline">\(y_i\)</span> (i.e. <span class="math inline">\(\hat{y_i}\)</span>) and the true value of <span class="math inline">\(y_i\)</span>
<ul>
<li><span class="math inline">\(e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)\)</span></li>
</ul></li>
<li>The sample regression line minimizes <span class="math inline">\(SS_{res}\)</span>
<span class="math display">\[
\begin{align*}
SS_{res}&amp;=e_1^2 + e_2^2 + \cdots + e_n^2 = \sum_{i=1}^n e_i^2\\
&amp;=(y_1 - b_0 + b_1 x_1)^2 + (y_2 - b_0 + b_1 x_2)^2 + \cdots + (y_n - b_0 + b_1 x_n)^2\\
&amp;= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\\
\end{align*}
\]</span></li>
</ul>
</div>
<div id="least-squares-estimate-lse" class="section level4">
<h4>Least Squares Estimate (LSE)</h4>
<p>With the least squares estimate approach we choose <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> to minimize <span class="math inline">\(SS_{res}\)</span></p>
<p><span class="math display">\[
(b_0, b_1) = \text{arg} \; \underset {\beta_0, \beta_1}{\text{min}} \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2
\]</span>
Taking the derivative and set them equal to 0 to arrive at the <strong>normal equations</strong>:</p>
<p><span class="math display">\[ 
\begin{align}
\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} &amp;= -2\sum_{i=1}^n (y_i-b_0-b_1x_i)\\
\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} &amp;= -2\sum_{i=1}^n x_i(y_i-b_0-b_1x_i)
\end{align}
\]</span>
#### Least Squares Estimates: Solving for _0 and _1</p>
<ul>
<li><p>Solving for <span class="math inline">\(\beta_0\)</span> given <span class="math inline">\(b_1\)</span>:
<span class="math display">\[b_0 = \overline{y} - b_1 \overline {x}\]</span></p></li>
<li><p>Solving for <span class="math inline">\(\beta_1\)</span> given that <span class="math inline">\(b_0 = \overline{y} - b_1 \overline {x}\)</span>:
<span class="math display">\[
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} = 
\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
=\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=
\frac{S_{xy}}{S_{xx}}
\end{align*}
\]</span></p></li>
</ul>
</div>
</div>
<div id="properties-of-least-square-estimators" class="section level3">
<h3>Properties of Least Square Estimators</h3>
<div id="b_0-and-b_1-are-linear-combinations-of-y_i" class="section level4">
<h4>1. <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are linear combinations of <span class="math inline">\(y_i\)</span></h4>
<p>for example for <span class="math inline">\(b_1\)</span> we use one of the forms:
<span class="math display">\[
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
\end{align*}
\]</span>
To clean up the equation we use the term <span class="math inline">\(c_i = \frac{x_i-\overline{x}}{S_{xx}}\)</span> for <span class="math inline">\(i=1,2,\dots n\)</span> and the equation becomes simply a constant multiplied by <span class="math inline">\(y_i\)</span>:
<span class="math display">\[
\begin{align*}
b_1 = \sum_{i=1}^ny_i c_i
\end{align*}
\]</span></p>
</div>
<div id="b_0-and-b_1-are-unbiased-estimators-of-beta_0-and-beta_1" class="section level4">
<h4>2. <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></h4>
<p>To show that <span class="math inline">\(b_0\)</span> is an unbiased estimator we want to show that <span class="math inline">\(E(b_0)=\beta_0\)</span>. Let us simply start by substituting in the value for <span class="math inline">\(b_0\)</span> from the normal equations.</p>
<p><span class="math display">\[
E(b_0) = E(\overline{y} - b_1\overline{x})
\]</span>
We use the helper function <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n} {y_i}={\overline{y}}\)</span> and pull out any values from the sum or Expected value function that are not dependent on them
<span class="math display">\[
E(b_0) = \frac{1}{n}\sum_{i=1}^n E({y_i}) - E(b_1)\overline{x}
\]</span>
Now substitute our equation for <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i\)</span> and <span class="math inline">\(E(b_1)=\beta_1\)</span> in addition to the fact that <span class="math inline">\(\beta_0, \beta_1, x_i\)</span> are constant values the Expected Value of those individual items is simply the value of those items…
<span class="math display">\[
E(b_0) = \frac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 x_i) - \beta_1 \overline{x}
\]</span>
Again we use a helper function for <span class="math inline">\(\sum_{i=1}^{n} {x_i}=n{\overline{x}}\)</span>
<span class="math display">\[
E(b_0) = \frac{1}{n}(n\beta_0 + n\beta_1 \overline{x}) - \beta_1 \overline{x}
\]</span>
Combining like terms we are left with simply:
<span class="math display">\[
E(b_0) = \beta_0 
\]</span></p>
</div>
</div>
