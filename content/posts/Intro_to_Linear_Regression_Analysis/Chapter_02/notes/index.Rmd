---
title: Intro. to Linear Regression Analysis - Chapter 2 - Simple Linear Regression
author: Jeremy Buss
date: '2021-09-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---
```{r echo=FALSE}
library(blogdown)
```
# Simple Linear Regression

The characteristics of a **Simple Linear Regression** are as follows:

* One predictor $X$ that is known and constant
* One response variable $Y$
* Linear function $y=\beta_0 + \beta_1 x$

<br>


## Simple Linear Regression - Population

The above is an idealized representation. Instead we have a set of n-pairs of data $(x_i, y_i)$ where $i=1, 2, ... n$. Additionally, the data will not perfectly fit the line. As a result we add an error term $\epsilon$. 

* Linear function  $Y_i=\beta_0 + \beta_1 X_i + \epsilon_i$ for $i=1,2,...n$
* Error term $\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)$
  * [$iid$](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables): identically, and independently distributed
  * $E(\epsilon_i)=0$
  * $Var(\epsilon_i)=\sigma^2$
* $Y_i$ is the $i$-th value of the response
* $X_i$ is the $i$-th value of the fixed value of the predictor
* $\beta_0, \beta_1,$ are model coefficients
* $\beta_0, \beta_1,$ and $\sigma^2$ are fixed unknown parameters of the model
* The mean response $\mu_{Y|X}=E(Y|X)$ has a straightline relationship with $X$ given by the population regression line
$$
\begin{align*}
\mu_{Y_i|X_i}=E(Y_i|X_i)&=E(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=\beta_0+\beta_1 X_i + E(\epsilon_i)\\
&=\beta_0+\beta_1 X_i
\end{align*}
$$
* The variance of $Y$ does not depend on $X$
$$
\begin{align*}
Var(Y_i|X_i) &= Var(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=Var(\epsilon_i)
\end{align*}
$$

* Any fixed value of $X_i=x$, the response variable $Y_i$ varies with $N(\mu_{Y_i|X_i=x},\sigma^2)$
  * $Y_i|X_i = N(\beta_0 + \beta_1 X_i, \sigma^2)$


## Parameter Estimation and Model Fitting

Given that we have sample data $\{(x_1, y_1),(x_2, y_2),...(x_n,y_n)\}$ we wish to find the _best_ sample regression line.

We are interested in the values of $\beta_0$ and $\beta_1$ in the following _sample_ regression model:
$$
\begin{align*}
y_i &= f(x_i) + \epsilon_i\\
&= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}
$$

or using sample statistics $b_0$ and $b_1$ computed from the training data to estimate them: 
$$\hat{y_i} = b_0 + b_1 x_i$$
where $\hat{y_i}$ is a point estimate of $y_i$ with mean $\mu_{Y|X=x_i}=E(Y|X=x_i)$

## Ordinary Least Squares (OLS)

We choose $b_0$ and $b_1$ that minimize the **sum of squared residuals** $SS_{res}$ or **residual sum of squares** $RSS$

* **residual** $e_i$ is a point estimate of $\epsilon_i$ and defined as the difference between our estimate of $y_i$ (i.e. $\hat{y_i}$) and the true value of $y_i$
  * $e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)$
* The sample regression line minimizes $SS_{res}$
$$
\begin{align*}
SS_{res}&=e_1^2 + e_2^2 + \cdots + e_n^2 = \sum_{i=1}^n e_i^2\\
&=(y_1 - b_0 + b_1 x_1)^2 + (y_2 - b_0 + b_1 x_2)^2 + \cdots + (y_n - b_0 + b_1 x_n)^2\\
&= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\\
\end{align*}
$$

## Least Squares Estimate (LSE)

With the least squares estimate approach we choose $b_0$ and $b_1$ to minimize $SS_{res}$

$$
(b_0, b_1) = \text{arg} \; \underset {\beta_0, \beta_1}{\text{min}} \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2
$$
Taking the derivative and set them equal to 0 to arrive at the **normal equations**:

$$ 
\begin{align}
\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} &= -2\sum_{i=1}^n (y_i-b_0-b_1x_i)\\
\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} &= -2\sum_{i=1}^n x_i(y_i-b_0-b_1x_i)
\end{align}
$$

### Least Squares Estimates: Solving for $\beta_0$ and $\beta_1$

* Solving for $\beta_0$ given $b_1$:
$$b_0 = \overline{y} - b_1 \overline {x}$$

* Solving for $\beta_1$ given that $b_0 = \overline{y} - b_1 \overline {x}$:
$$
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} = 
\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
=\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=
\frac{S_{xy}}{S_{xx}}
\end{align*}
$$

### Best Linear Unbiased Estimators (BLUE)

#### 1. Best:

For the estimators to be the "best" they need to have minimum variance. 

##### Variance of $\beta_1$

$$
\begin{align*}
Var(b_1)&=Var \left (\sum_{i=1}^n c_i y_i \right ) = \sum_{i=1}^n c_i^2 Var(y_i)
\end{align*}
$$

Since the observations $y_i$ are uncorrelated then the variance of the sum is just the sum of the variances. Since the $Var(y_i)=\sigma^2$

$$
\begin{align*}
Var(b_1)&= \sigma^2 \sum_{i=1}^n c_i^2 = \frac {\sigma^2 \sum_{i=1}^n (x_i-\overline {x})^2}{S_{xx}} = \frac{\sigma^2}{S_{xx}}
\end{align*}
$$

##### The Variance of $b_0$

$$
\begin{align*}
Var(b_0) &= Var(\overline{y} - b_1 \overline {x})
\end{align*}
$$
Using the [properties of variance](https://en.wikipedia.org/wiki/Variance#Properties) for the difference of two random variables this becomes:
$$
\begin{align}
Var(b_0) &= Var(\overline{y}) + \overline{x}^2 Var (b_1) - 2 \overline{x} Cov(\overline{y},b_1)
\end{align}
$$
To simplify this further we need to find $Var(\overline{y})$

$$
\begin{align*}
Var(\overline {y}) &= Var \left ( \frac{y_1+y_2+\dots+y_n}{n} \right )\\
&= \left ( \frac {1}{n} \right )^2 (y_1+y_2+\dots+y_n)\\
&= \left ( \frac {1}{n} \right )^2 (\sigma^2+\sigma^2+\dots+\sigma^2)\\
&= \left ( \frac {1}{n} \right )^2 n \sigma^2\\
&= \frac {\sigma^2}{n} 
\end{align*}
$$
Additionally we can simplify the last term in equation #3 and focus on the Covariance.

Let us start by simply filling in the values for $\overline{y}$ and $b_1$.

$$
Cov(\overline{y},b_1)=
Cov\left(\frac{1}{n}\sum_{i=1}^ny_i,    \frac {\sum_{i=1}^ny_i(x_i-\overline{x})} {S_{xx}} \right)
$$
A constant can be pulled out of the covariance and simply multiplied. 
$$
Cov(\overline{y},b_1)=
\frac{1}{n}Cov\left(\sum_{i=1}^ny_i,    \frac {\sum_{i=1}^ny_i(x_i-\overline{x})} {S_{xx}} \right)
$$
Next we let $c_i = \frac{(x_i - \overline{x})}{S_{xx}}$:


$$
Cov(\overline{y},b_1)=
\frac{1}{n}Cov\left(\sum_{i=1}^ny_i,    \sum_{i=1}^nc_iy_i \right)
$$

Now since $\overline{y}$ and $b_1$ are independently distributed normal variables we can pull the $c_i$ term out of the $Cov$

$$
\begin{split}
Cov(\overline{y},b_1) =
\frac{1}{n}\sum_{i=1}^nc_i \cdot Cov\left(\sum_{i=1}^ny_i,    \sum_{i=1}^ny_i \right)\\
\\
Cov(\overline{y},b_1)= \frac{1}{n}\sum_{i=1}^nc_i \cdot Var(y_i)\\
\\
Cov(\overline{y},b_1)= \frac{\sigma^2}{n}\sum_{i=1}^nc_i\\
\end{split}
$$
Now if we remember that $c_i = \frac{(x_i - \overline{x})}{S_{xx}}$ and sum of which will be 0 so:
$$
\begin{split}
Cov(\overline{y},b_1)&= \frac{\sigma^2}{n}\sum_{i=1}^n \frac{(x_i - \overline{x})}{S_{xx}}\\
\\
Cov(\overline{y},b_1)&= \frac{\sigma^2}{n} \cdot 0\\
\\
Cov(\overline{y},b_1)&= 0\\
\end{split}
$$
So we see that the last term in equation #3 is 0. So we are left with:

$$
\begin{align*}
Var(b_0) &= Var(\overline{y}) + \overline{x}^2 Var (b_1)\\
&= Var(\overline{y}) + \overline{x}^2 Var (b_1)\\
&= \frac{\sigma^2}{n} + \frac{\overline{x}^2\sigma^2}{S_{xx}}\\
Var(b_0)&= \sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \right )
\end{align*}
$$

#### 2. Linear:




$b_0$ and $b_1$ are linear combinations of $y_i$ for example for $b_1$ we use one of the forms:
$$
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
\end{align*}
$$
To clean up the equation we use the term $c_i = \frac{x_i-\overline{x}}{S_{xx}}$ for $i=1,2,\dots n$ and the equation becomes simply a constant multiplied by $y_i$:
$$
\begin{align*}
b_1 = \sum_{i=1}^ny_i c_i
\end{align*}
$$

#### 3. Unbiased

$b_0$ and $b_1$ are unbiased estimators of $\beta_0$ and $\beta_1$

For $b_0$ to be an unbiased estimator we want to show that $E(b_0)=\beta_0$. Let us simply start by substituting in the value for $b_0$ from the normal equations.

$$
E(b_0) = E(\overline{y} - b_1\overline{x})
$$
We use the helper function $\frac{1}{n}\sum_{i=1}^{n} {y_i}={\overline{y}}$ and pull out any values from the sum or Expected value function that are not dependent on them
$$
E(b_0) = \frac{1}{n}\sum_{i=1}^n E({y_i}) - E(b_1)\overline{x}
$$
Now substitute our equation for $y_i = \beta_0 + \beta_1 x_i$ and $E(b_1)=\beta_1$ in addition to the fact that $\beta_0, \beta_1, x_i$ are constant values the Expected Value of those individual items is simply the value of those items...
$$
E(b_0) = \frac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 x_i) - \beta_1 \overline{x}
$$
Again we use a helper function for $\sum_{i=1}^{n} {x_i}=n{\overline{x}}$
$$
E(b_0) = \frac{1}{n}(n\beta_0 + n\beta_1 \overline{x}) - \beta_1 \overline{x}
$$
Combining like terms we are left with simply:
$$
E(b_0) = \beta_0 
$$

Similarly for $\beta_1$:

$$
\begin{align*}
E(b_1)&=E \left (\sum_{i=1}^n c_i y_i \right ) = \sum_{i=1}^n c_i E(y_i)\\
&=\sum_{i=1}^n c_i (b_0 + b_1 x_i)=b_0 \sum_{i=1}^n c_i + b_1 \sum_{i=1}^n c_i x_i
\end{align*}
$$

Now $\sum_{i=1}^n c_i =0$ and $\sum_{i=1}^n c_i x_i =1$ so
$$
E(b_1)=\beta_1
$$

#### Properties of Least Squares Fit

1. Sum of residuals is zero $\sum_{i=1}^n(y_i - \hat {y_i}) = \sum_{i=1}^n e_i = 0$
2. The sum of observations $y_i$ and the sum of fitted values $\hat{y_i}$ are the same $\sum_{i=1}^ny_i = \sum_{i=1}^n \hat{y}$
3. The Least Squares Regression line passes through the centroid point $(\overline{x},\overline{y})$
4. Inner product of residual and predictor is zero $\sum_{i=1}^n e_i x_i = 0$
5. Inner product of residual and predicted values is zero $\sum_{i=1}^n e_i \hat{y_i} = 0$

#### Estimation for $\sigma^2$

The **error** or **residual sum of squares** is:
$$SS_{res} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2$$

The residual sum of squares has $n-2$ degrees of freedom, because two degrees of freedom are associated with the estimators $b_0$ and $b_1$ involved in obtaining $\hat{y_i}$. Dividing the residual sum of squares by the degrees of freedom we arrive at the **residual mean square**

$$\hat{\sigma}^2 = \frac {SS_{Res}}{n-2}=MS_{Res}$$
$\hat{\sigma}^2$ is an unbiased estimator for $\sigma^2$, i.e. $E(MS_{Res})=\sigma^2$

### Interval Estimation

We know that the errors $\epsilon_i$ are $NID(0,\sigma^2)$ and the observations $y_i$ are $NID(\beta_0 + \beta_1 x_i, \sigma^2)$. Also, $b_0$ and $b_1$ are linear combinations of $y_i$. Therefore $b_0$ and $b_1$ are normally distributed. We can synthesize the information this way:

$$
\begin{align*}
b_1 & \sim NID \left ( \beta_1,\frac{\sigma^2}{S_{xx}} \right )\\
b_0 & \sim NID \left ( \beta_0,\sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}}\right ) \right )
\end{align*}
$$
Since we just said that they are normal we could standardize them:

$$
\begin{align*}
\frac {b_1 - \beta_1}{\sqrt{\sigma^2/S_{xx}}} & \sim NID (0,1)\\
\frac {b_0 - \beta_0}{\sqrt{ \sigma^2 (1/n + \overline{x}^2/S_{xx})}} & \sim NID (0,1)\\
\end{align*}
$$
However, we don't know the true value of $\sigma^2$. Therefore we can use the estimate of $\sigma$ that we found earlier: $\hat{\sigma}^2$.

$$
\begin{align*}
\frac {b_1 - \beta_1}{\sqrt{\hat{\sigma}^2/S_{xx}}} & \sim t_{n-2}\\
\frac {b_0 - \beta_0}{\sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}} & \sim t_{n-2}\\
\end{align*}
$$
We can use this information to construct a $(1-\alpha)100%$ confidence intervals:

* $\beta_1$: $b_1 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2/S_{xx}}$
* $\beta_0$: $b_0 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2/S_{xx}}$
