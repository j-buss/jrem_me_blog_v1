---
title: Intro. to Linear Regression Analysis - Chapter 2 - Simple Linear Regression
author: Jeremy Buss
date: '2021-09-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---

### Simple Linear Regression

The characteristics of a **Simple Linear Regression** are as follows:

* One predictor $X$ that is known and constant
* One response variable $Y$
* Linear function $y=\beta_0 + \beta_1 x$

<br>


#### Simple Linear Regression - Population

The above is an idealized representation. Instead we have a set of n-pairs of data $(x_i, y_i)$ where $i=1, 2, ... n$. Additionally, the data will not perfectly fit the line. As a result we add an error term $\epsilon$. 

* Linear function  $Y_i=\beta_0 + \beta_1 X_i + \epsilon_i$ for $i=1,2,...n$
* Error term $\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)$
  * [$iid$](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables): identically, and independently distributed
  * $E(\epsilon_i)=0$
  * $Var(\epsilon_i)=\sigma^2$
* $Y_i$ is the $i$-th value of the response
* $X_i$ is the $i$-th value of the fixed value of the predictor
* $\beta_0, \beta_1,$ are model coefficients
* $\beta_0, \beta_1,$ and $\sigma^2$ are fixed unknown parameters of the model
* The mean response $\mu_{Y|X}=E(Y|X)$ has a straightline relationship with $X$ given by the population regression line
$$
\begin{align*}
\mu_{Y_i|X_i}=E(Y_i|X_i)&=E(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=\beta_0+\beta_1 X_i + E(\epsilon_i)\\
&=\beta_0+\beta_1 X_i
\end{align*}
$$
* The variance of $Y$ does not depend on $X$
$$
\begin{align*}
Var(Y_i|X_i) &= Var(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=Var(\epsilon_i)
\end{align*}
$$

* Any fixed value of $X_i=x$, the response variable $Y_i$ varies with $N(\mu_{Y_i|X_i=x},\sigma^2)$
  * $Y_i|X_i = N(\beta_0 + \beta_1 X_i, \sigma^2)$


### Parameter Estimation and Model Fitting

Given that we have sample data $\{(x_1, y_1),(x_2, y_2),...(x_n,y_n)\}$ we wish to find the _best_ sample regression line.

We are interested in the values of $\beta_0$ and $\beta_1$ in the following _sample_ regression model:
$$
\begin{align*}
y_i &= f(x_i) + \epsilon_i\\
&= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}
$$

or using sample statistics $b_0$ and $b_1$ computed from the training data to estimate them: 
$$\hat{y_i} = b_0 + b_1 x_i$$
where $\hat{y_i}$ is a point estimate of $y_i$ with mean $\mu_{Y|X=x_i}=E(Y|X=x_i)$

#### Ordinary Least Squares (OLS)

We choose $b_0$ and $b_1$ that minimize the **sum of squared residuals** $SS_{res}$ or **residual sum of squares** $RSS$

* **residual** $e_i$ is a point estimate of $\epsilon_i$ and defined as the difference between our estimate of $y_i$ (i.e. $\hat{y_i}$) and the true value of $y_i$
  * $e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)$
* The sample regression line minimizes $SS_{res}$
$$
\begin{align*}
SS_{res}&=e_1^2 + e_2^2 + \cdots + e_n^2 = \sum_{i=1}^n e_i^2\\
&=(y_1 - b_0 + b_1 x_1)^2 + (y_2 - b_0 + b_1 x_2)^2 + \cdots + (y_n - b_0 + b_1 x_n)^2\\
&= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\\
\end{align*}
$$

#### Least Squares Estimate (LSE)

With the least squares estimate approach we choose $b_0$ and $b_1$ to minimize $SS_{res}$

$$
(b_0, b_1) = \text{arg} \; \underset {\beta_0, \beta_1}{\text{min}} \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2
$$
Taking the derivative and setting equal to 0:

$$ 
\begin{align*}
\frac {3}{4}\bigg\vert \\
\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} &= \sum_{i=1}^n -2(y_i-b_0-b_1x_i)\\
\\
\frac{\partial SS_{res}}{\partial {\beta_1}} &= \sum_{i=1}^n -2x_i(y_i-b_0-b_1x_i)
\end{align*}
$$