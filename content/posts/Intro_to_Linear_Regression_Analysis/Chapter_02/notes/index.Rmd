---
title: Simple Linear Regression
author: Jeremy Buss
date: '2021-09-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---
```{r echo=FALSE}
library(blogdown)
```
# Simple Linear Regression

The characteristics of a **Simple Linear Regression** are as follows:

* One predictor $X$ that is known and constant
* One response variable $Y$
* Linear function $y=\beta_0 + \beta_1 x$

<br>


## Simple Linear Regression - Population

The above is an idealized representation. Instead we have a set of n-pairs of data $(x_i, y_i)$ where $i=1, 2, ... n$. Additionally, the data will not perfectly fit the line. As a result we add an error term $\epsilon$. 

* Linear function  $Y_i=\beta_0 + \beta_1 X_i + \epsilon_i$ for $i=1,2,...n$
* Error term $\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)$
  * [$iid$](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables): identically, and independently distributed
  * $E(\epsilon_i)=0$
  * $Var(\epsilon_i)=\sigma^2$
* $Y_i$ is the $i$-th value of the response
* $X_i$ is the $i$-th value of the fixed value of the predictor
* $\beta_0, \beta_1,$ are model coefficients
* $\beta_0, \beta_1,$ and $\sigma^2$ are fixed unknown parameters of the model
* The mean response $\mu_{Y|X}=E(Y|X)$ has a straightline relationship with $X$ given by the population regression line
$$
\begin{align*}
\mu_{Y_i|X_i}=E(Y_i|X_i)&=E(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=\beta_0+\beta_1 X_i + E(\epsilon_i)\\
&=\beta_0+\beta_1 X_i
\end{align*}
$$
* The variance of $Y$ does not depend on $X$
$$
\begin{align*}
Var(Y_i|X_i) &= Var(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=Var(\epsilon_i)
\end{align*}
$$

* Any fixed value of $X_i=x$, the response variable $Y_i$ varies with $N(\mu_{Y_i|X_i=x},\sigma^2)$
  * $Y_i|X_i = N(\beta_0 + \beta_1 X_i, \sigma^2)$


## Parameter Estimation and Model Fitting

Given that we have sample data $\{(x_1, y_1),(x_2, y_2),...(x_n,y_n)\}$ we wish to find the _best_ sample regression line.

We are interested in the values of $\beta_0$ and $\beta_1$ in the following _sample_ regression model:
$$
\begin{align*}
y_i &= f(x_i) + \epsilon_i\\
&= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}
$$

or using sample statistics $b_0$ and $b_1$ computed from the training data to estimate them: 
$$\hat{y_i} = b_0 + b_1 x_i$$
where $\hat{y_i}$ is a point estimate of $y_i$ with mean $\mu_{Y|X=x_i}=E(Y|X=x_i)$

## Ordinary Least Squares (OLS)

We choose $b_0$ and $b_1$ that minimize the **sum of squared residuals** $SS_{res}$ or **residual sum of squares** $RSS$

* **residual** $e_i$ is a point estimate of $\epsilon_i$ and defined as the difference between our estimate of $y_i$ (i.e. $\hat{y_i}$) and the true value of $y_i$
  * $e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)$
* The sample regression line minimizes $SS_{res}$
$$
\begin{align*}
SS_{res}&=e_1^2 + e_2^2 + \cdots + e_n^2 = \sum_{i=1}^n e_i^2\\
&=(y_1 - b_0 + b_1 x_1)^2 + (y_2 - b_0 + b_1 x_2)^2 + \cdots + (y_n - b_0 + b_1 x_n)^2\\
&= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\\
\end{align*}
$$

## Least Squares Estimate (LSE)

With the least squares estimate approach we choose $b_0$ and $b_1$ to minimize $SS_{res}$

$$
(b_0, b_1) = \text{arg} \; \underset {\beta_0, \beta_1}{\text{min}} \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2
$$
Taking the derivative and set them equal to 0 to arrive at the **normal equations**:

$$ 
\begin{align}
\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} &= -2\sum_{i=1}^n (y_i-b_0-b_1x_i)\\
\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} &= -2\sum_{i=1}^n x_i(y_i-b_0-b_1x_i)
\end{align}
$$

### Least Squares Estimates: Solving for $\beta_0$ and $\beta_1$

* Solving for $\beta_0$ given $b_1$:
$$b_0 = \overline{y} - b_1 \overline {x}$$

* Solving for $\beta_1$ given that $b_0 = \overline{y} - b_1 \overline {x}$:
$$
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} = 
\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
=\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=
\frac{S_{xy}}{S_{xx}}
\end{align*}
$$

### Best Linear Unbiased Estimators (BLUE)

#### 1. Best:

For the estimators to be the "best" they need to have minimum variance. 

##### Variance of $\beta_1$

$$
\begin{align*}
Var(b_1)&=Var \left (\sum_{i=1}^n c_i y_i \right ) = \sum_{i=1}^n c_i^2 Var(y_i)
\end{align*}
$$

Since the observations $y_i$ are uncorrelated then the variance of the sum is just the sum of the variances. Since the $Var(y_i)=\sigma^2$

$$
\begin{align*}
Var(b_1)&= \sigma^2 \sum_{i=1}^n c_i^2 = \frac {\sigma^2 \sum_{i=1}^n (x_i-\overline {x})^2}{S_{xx}} = \frac{\sigma^2}{S_{xx}}
\end{align*}
$$

##### The Variance of $b_0$

$$
\begin{align*}
Var(b_0) &= Var(\overline{y} - b_1 \overline {x})
\end{align*}
$$
Using the [properties of variance](https://en.wikipedia.org/wiki/Variance#Properties) for the difference of two random variables this becomes:
$$
\begin{align}
Var(b_0) &= Var(\overline{y}) + \overline{x}^2 Var (b_1) - 2 \overline{x} Cov(\overline{y},b_1)
\end{align}
$$
To simplify this further we need to find $Var(\overline{y})$

$$
\begin{align*}
Var(\overline {y}) &= Var \left ( \frac{y_1+y_2+\dots+y_n}{n} \right )\\
&= \left ( \frac {1}{n} \right )^2 (y_1+y_2+\dots+y_n)\\
&= \left ( \frac {1}{n} \right )^2 (\sigma^2+\sigma^2+\dots+\sigma^2)\\
&= \left ( \frac {1}{n} \right )^2 n \sigma^2\\
&= \frac {\sigma^2}{n} 
\end{align*}
$$
Additionally we can simplify the last term in equation #3 and focus on the Covariance.

Let us start by simply filling in the values for $\overline{y}$ and $b_1$.

$$
Cov(\overline{y},b_1)=
Cov\left(\frac{1}{n}\sum_{i=1}^ny_i,    \frac {\sum_{i=1}^ny_i(x_i-\overline{x})} {S_{xx}} \right)
$$
A constant can be pulled out of the covariance and simply multiplied. 
$$
Cov(\overline{y},b_1)=
\frac{1}{n}Cov\left(\sum_{i=1}^ny_i,    \frac {\sum_{i=1}^ny_i(x_i-\overline{x})} {S_{xx}} \right)
$$
Next we let $c_i = \frac{(x_i - \overline{x})}{S_{xx}}$:


$$
Cov(\overline{y},b_1)=
\frac{1}{n}Cov\left(\sum_{i=1}^ny_i,    \sum_{i=1}^nc_iy_i \right)
$$

Now since $\overline{y}$ and $b_1$ are independently distributed normal variables we can pull the $c_i$ term out of the $Cov$

$$
\begin{split}
Cov(\overline{y},b_1) =
\frac{1}{n}\sum_{i=1}^nc_i \cdot Cov\left(\sum_{i=1}^ny_i,    \sum_{i=1}^ny_i \right)\\
\\
Cov(\overline{y},b_1)= \frac{1}{n}\sum_{i=1}^nc_i \cdot Var(y_i)\\
\\
Cov(\overline{y},b_1)= \frac{\sigma^2}{n}\sum_{i=1}^nc_i\\
\end{split}
$$
Now if we remember that $c_i = \frac{(x_i - \overline{x})}{S_{xx}}$ and sum of which will be 0 so:
$$
\begin{split}
Cov(\overline{y},b_1)&= \frac{\sigma^2}{n}\sum_{i=1}^n \frac{(x_i - \overline{x})}{S_{xx}}\\
\\
Cov(\overline{y},b_1)&= \frac{\sigma^2}{n} \cdot 0\\
\\
Cov(\overline{y},b_1)&= 0\\
\end{split}
$$
So we see that the last term in equation #3 is 0. So we are left with:

$$
\begin{align*}
Var(b_0) &= Var(\overline{y}) + \overline{x}^2 Var (b_1)\\
&= Var(\overline{y}) + \overline{x}^2 Var (b_1)\\
&= \frac{\sigma^2}{n} + \frac{\overline{x}^2\sigma^2}{S_{xx}}\\
Var(b_0)&= \sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \right )
\end{align*}
$$

#### 2. Linear:




$b_0$ and $b_1$ are linear combinations of $y_i$ for example for $b_1$ we use one of the forms:
$$
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
\end{align*}
$$
To clean up the equation we use the term $c_i = \frac{x_i-\overline{x}}{S_{xx}}$ for $i=1,2,\dots n$ and the equation becomes simply a constant multiplied by $y_i$:
$$
\begin{align*}
b_1 = \sum_{i=1}^ny_i c_i
\end{align*}
$$

#### 3. Unbiased

$b_0$ and $b_1$ are unbiased estimators of $\beta_0$ and $\beta_1$

For $b_0$ to be an unbiased estimator we want to show that $E(b_0)=\beta_0$. Let us simply start by substituting in the value for $b_0$ from the normal equations.

$$
E(b_0) = E(\overline{y} - b_1\overline{x})
$$
We use the helper function $\frac{1}{n}\sum_{i=1}^{n} {y_i}={\overline{y}}$ and pull out any values from the sum or Expected value function that are not dependent on them
$$
E(b_0) = \frac{1}{n}\sum_{i=1}^n E({y_i}) - E(b_1)\overline{x}
$$
Now substitute our equation for $y_i = \beta_0 + \beta_1 x_i$ and $E(b_1)=\beta_1$ in addition to the fact that $\beta_0, \beta_1, x_i$ are constant values the Expected Value of those individual items is simply the value of those items...
$$
E(b_0) = \frac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 x_i) - \beta_1 \overline{x}
$$
Again we use a helper function for $\sum_{i=1}^{n} {x_i}=n{\overline{x}}$
$$
E(b_0) = \frac{1}{n}(n\beta_0 + n\beta_1 \overline{x}) - \beta_1 \overline{x}
$$
Combining like terms we are left with simply:
$$
E(b_0) = \beta_0 
$$

Similarly for $\beta_1$:

$$
\begin{align*}
E(b_1)&=E \left (\sum_{i=1}^n c_i y_i \right ) = \sum_{i=1}^n c_i E(y_i)\\
&=\sum_{i=1}^n c_i (b_0 + b_1 x_i)=b_0 \sum_{i=1}^n c_i + b_1 \sum_{i=1}^n c_i x_i
\end{align*}
$$

Now $\sum_{i=1}^n c_i =0$ and $\sum_{i=1}^n c_i x_i =1$ so
$$
E(b_1)=\beta_1
$$

#### Properties of Least Squares Fit

1. Sum of residuals is zero $\sum_{i=1}^n(y_i - \hat {y_i}) = \sum_{i=1}^n e_i = 0$
2. The sum of observations $y_i$ and the sum of fitted values $\hat{y_i}$ are the same $\sum_{i=1}^ny_i = \sum_{i=1}^n \hat{y}$
3. The Least Squares Regression line passes through the centroid point $(\overline{x},\overline{y})$
4. Inner product of residual and predictor is zero $\sum_{i=1}^n e_i x_i = 0$
5. Inner product of residual and predicted values is zero $\sum_{i=1}^n e_i \hat{y_i} = 0$

#### Estimation for $\sigma^2$

The **error** or **residual sum of squares** is:
$$SS_{res} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2$$

The residual sum of squares has $n-2$ degrees of freedom, because two degrees of freedom are associated with the estimators $b_0$ and $b_1$ involved in obtaining $\hat{y_i}$. Dividing the residual sum of squares by the degrees of freedom we arrive at the **residual mean square**

$$\hat{\sigma}^2 = \frac {SS_{Res}}{n-2}=MS_{Res}$$
$\hat{\sigma}^2$ is an unbiased estimator for $\sigma^2$, i.e. $E(MS_{Res})=\sigma^2$

### Sampling Distribution

We do not know the true values of $\beta_0$ or $\beta_1$. Therefore we must derive them from the data. As such we must take into account how these values are derived and what the likelihood of the true value being within a certain range.

#### Sampling Distribution of $\beta_0$ and $\beta_1$

We know that the errors $\epsilon_i$ are $NID(0,\sigma^2)$ and the observations $y_i$ are $NID(\beta_0 + \beta_1 x_i, \sigma^2)$. Also, $b_0$ and $b_1$ are linear combinations of $y_i$. Therefore $b_0$ and $b_1$ are normally distributed. We can synthesize the information this way:

$$
\begin{align*}
b_1 & \sim NID \left ( \beta_1,\frac{\sigma^2}{S_{xx}} \right )\\
b_0 & \sim NID \left ( \beta_0,\sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}}\right ) \right )
\end{align*}
$$
Since we just said that they are normal we could standardize them:

$$
\begin{align*}
\frac {b_1 - \beta_1}{\sqrt{\sigma^2/S_{xx}}} & \sim NID (0,1)\\
\frac {b_0 - \beta_0}{\sqrt{ \sigma^2 (1/n + \overline{x}^2/S_{xx})}} & \sim NID (0,1)\\
\end{align*}
$$
However, we don't know the true value of $\sigma^2$. Therefore we can use the estimate of $\sigma$ that we found earlier: $\hat{\sigma}^2$.

$$
\begin{align}
\frac {b_1 - \beta_1}{\sqrt{\hat{\sigma}^2/S_{xx}}} & \sim t_{n-2}\\
\frac {b_0 - \beta_0}{\sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}} & \sim t_{n-2}\\
\end{align}
$$
We can use this information to construct a $(1-\alpha)100%$ confidence intervals:

* $\beta_1$: $b_1 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2/S_{xx}}$
* $\beta_0$: $b_0 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}$

#### Sampling Distribution of $\hat{\sigma^2}$

It can be shown that $\frac {SS_{res}}{\sigma^2}$  follows a chi-squared distribution. We can use that fact to the sampling distribution of $\hat{\sigma^2}$:

$$
\begin{align*}
\frac {SS_{res}}{\sigma^2} & \sim \chi_{n-2}^2\\
SS_{res} & \sim \sigma^2 \chi_{n-2}^2\\
\frac {1}{n-2}SS_{res} & \sim \frac {1}{n-2} \sigma^2 \chi_{n-2}^2\\
\hat{\sigma^2} = MS_{res} & \sim \sigma^2 \frac {\chi_{n-2}^2}{n-2}  \\
\end{align*}
$$

We can use this information to construct a $(1-\alpha)100%$ confidence interval:

* $\sigma^2$: $\left ( \frac{SS_{res}}{{\chi_{\alpha/2,n-2}^2}}, \frac{SS_{res}}{{\chi_{1-\alpha/2,n-2}^2}}\right )$

### Hypothesis Testing

#### Standard Error

The denominator of the test statistics (equations #4 and #5) are often referred to as the **standard error**. Rewriting each of the terms as:

* $se(\hat{\beta_1}) = \sqrt{\frac{MS_{Res}}{S_{xx}}}$
* $se(\hat{\beta_0}) = \sqrt{MS_{Res} (1/n + \overline{x}^2/S_{xx})}$

#### Hypothesis Testing 

We reject $H_0$ in favor of $H_1$ if

* Critical Value Method: $\vert t_{test} \vert > t_{\alpha/2,n-2}$
* P-Value Method: p-value = $2P(t_{n-2} > \vert t_{test} \vert ) < \alpha$

##### $\beta_1$

* Null Hypothesis: $H_0$: $\beta_1=\beta_1^0$
* Alternative Hypothesis: $H_1$: $\beta_1 \ne \beta_1^0$
* Standard error: $se(\hat{\beta_1}) = \sqrt{\frac{MS_{Res}}{S_{xx}}}$
* Test statistic: $t_{test} = \frac {b_1 - \beta_1^0}{se(b_1)} \sim t_{n-2}$ under $H_0$

##### $\beta_0$

* Null Hypothesis: $H_0$: $\beta_0=\beta_0^0$
* Alternative Hypothesis: $H_1$: $\beta_0 \ne \beta_0^0$
* Standard error: $se(\hat{\beta_0}) = \sqrt{MS_{Res} (1/n + \overline{x}^2/S_{xx})}$
* Test statistic: $t_{test} = \frac {b_0 - \beta_0^0}{se(b_0)} \sim t_{n-2}$ under $H_0$

### Analysis of Variance (ANOVA)

The **analysis of variance** approach involves partitioning the total variability in the response variable y into different pieces. We start by examining the total variability.

If $x$ and $y$ are linearly related, but you have no information about $x$; How would you predict a value of $y$?

Well...we would use the average of $y$, or $\overline{y}$ as a prediction of $y$

And the difference between that estimate the the true value could be described as 

$$(y_i - \overline{y}) = (\hat{y_i} - \overline{y}) + (y_i - \hat{y_i}) $$
If we square both sides and sum over all observations:

$$
\sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n (\hat{y_i} - \overline{y})^2 + \sum_{i=1}^n (y_i - \hat{y_i})^2 + 2\sum_{i=1}^n (\hat{y_i} - \overline{y})(y_i - \hat{y_i})
$$
The third term in this last equation can be rewritten as:

$$\begin{align*}
2\sum_{i=1}^n (\hat{y_i} - \overline{y})(y_i - \hat{y_i}) &= 
2\sum_{i=1}^n \hat{y_i} (y_i - \hat{y_i}) - 
2\overline{y}\sum_{i=1}^n (y_i - \hat{y_i})\\
&=2\sum_{i=1}^n \hat{y_i} e_i - 
2\overline{y}\sum_{i=1}^n e_i\\
\end{align*}
$$
Now we can leverage two of the Properties of Least Squares Fit from above. Namely #1 where the sum of residuals is 0 and #5 where the inner product of residuals and predicted values is zero.

Therefore the entire third term is 0

$$\begin{align*}
2\sum_{i=1}^n (\hat{y_i} - \overline{y})(y_i - \hat{y_i}) &= 
2\sum_{i=1}^n \hat{y_i} e_i - 
2\overline{y}\sum_{i=1}^n e_i\\
&=0
\end{align*}
$$

So the total variability can be described as:

$$
\begin{align*}
\sum_{i=1}^n (y_i - \overline{y})^2 &= \sum_{i=1}^n (\hat{y_i} - \overline{y})^2 + \sum_{i=1}^n (y_i - \hat{y_i})^2\\
SS_T &= SS_R + SS_{Res}
\end{align*}
$$
Where the terms are:

* **Sum of Squares - Total** $(SS_T)$ : $\sum_{i=1}^n (y_i - \overline{y})^2$
* **Sum of Squares - Regression** $(SS_R)$: $\sum_{i=1}^n (\hat{y_i} - \overline{y})^2$
* **Sum of Squares - Residual** $(SS_Res)$: $\sum_{i=1}^n (y_i - \hat{y_i})^2$

Furthermore the degrees of freedom (df) are as follows:

* $\textcolor{blue}{df_T = n-1}$ Lose 1 df with constraint $\sum_{i=1}^n (y_i = \overline{y})=0$
* $\textcolor{blue}{df_R=1}$ all $\hat{y_i}$ are on the regression line with 2 df (intercept and slope), but with constraint $\sum_{i=1}^n (\hat{y_i} = \overline{y})=0$
* $\textcolor{blue}{df_{res}=n-2}$ Lose 2 dfs because $\beta_0$ and $\beta_1$ are estimated by $b_0$ and $b_1$ which are linear combo of $y_i$

#### Analysis of Variance F test

We can define an F statistic:

$$
F_{test}=\frac {SS_R/df_R}{SS_{Res}/df_{Res}}=\frac {SS_R/1}{SS_{Res}/{(n-2)}}=\frac{MS_R}{MS_{Res}}
$$
Therefore, to test the hypothesis $H_0: \beta_1 = 0$ compute $F_{test}$ and reject $H_0$ if 

* $F_{test} > F_{\alpha,1,n-2}$
* p-value = $P(F_{1,n-2} > F_{test}) > \alpha$

Combining all these definitions we have...


```{r echo=FALSE, results='asis'}
x <- data.frame("Source of Variation" = c("Regression","Residual","Total"),
                "Sum of Squares" = c("$SS_R=\\hat{\\beta_1}S_{xy}$","$SS_{Res}=SS_T - {\\beta_1}S_{xy}$","$SS_T$"),
                "Degrees of Freedom" = c("1", "n-2", "n-1"),
                "Mean Square" = c("$MS_R$", "$MS_{Res}$",""),
                "$F_0$" = c("$MS_R/MS_{Res}$", "","")
                )
knitr::kable(x=x,caption = "ANOVA Table",format="markdown", escape=FALSE, col.names=c("Source of Variation","Sum of Squares","Degrees of Freedom","Mean Square","$F_{test}$"))

```

In SLR, the $F$-test of ANOVA give the same result as a two-sided $t$-test of $H_0$:$\beta_1$=0

### Coefficient of Determination

The **coefficient of determination** is defined as the following:

$$
R^2 = \frac {SS_R}{SS_T}= \frac {SS_T - SS_{Res}}{SS_T} = 1-\frac {SS_{Res}}{SS_T}
$$

Since $SS_T$ is the measure of variability in y without considering the effect of the regressor variable $x$ and $SS_{Res}$ is a measure of the variability in $y$ remaining after $x$ has been considered.

$R^2$ is often called the proportion of variation explained by the regressor $x$.

### Prediction

#### Mean Response 

If $x_0$ is within the range of $x$, an unbiased point estimate of $E(y|x_0)$ is:

$$
\widehat{E(y|x_0)}=\hat{\mu_{y|x_0}}=b_0+b_1x_0
$$

The variance of $\hat{\mu_{y|x_0}}$ is:
$$
\begin{align*}
Var(\hat{\mu_{y|x_0}}) &= Var(b_0+b_1 x_0) = Var(\overline{y} + b_1(x_0-\overline{x}))\\
&=\frac{\sigma^2}{n}+ \frac{\sigma^2 (x_0 - \overline{x})^2}{S_{xx}}=\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right )
\end{align*}
$$

So the sampling distribution of $\hat{\mu_{y|x_0}}$ is:

$$
N \left ( \beta_0+\beta_1x_0,\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
So now we have
$$
(\widehat{\mu_{y|x_0}}=b_0+b_1 x_0)
\sim
N \left ( \beta_0+\beta_1 x_0,\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
And we take the steps to normalize the standard distribution:

$$
\frac {(b_0+b_1 x_0)-(\beta_0+\beta_1 x_0)}{\sigma \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim NID(0,1)
$$

We don't know the value of $\sigma$ so we use the "hatted" version and use a $t$ distribution:

$$
\frac {(b_0+b_1 x_0)-(\beta_0+\beta_1 x_0)}{\hat{\sigma} \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim t_{n-2}
$$

So we have the $(1-\alpha)100%$ Confidence interval for $E(y|x_0)$ is:

$$
\hat{\mu_{y|x_0}} \pm t_{\alpha/2,n-2} 
\hat{\sigma} \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}
$$

Or written slightly differently using the relationship of $\hat{\sigma^2} = MS_{Res}$:
$$
\hat{\mu_{y|x_0}} \pm t_{\alpha/2,n-2} 
\sqrt{MS_{Res} \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right )}
$$

#### Point Estimate

The sampling distribution of $\hat{y_0}$ is:

$$
\hat{y_0}=b_0+b_1 x_0 \sim 
N \left ( \beta_0+\beta_1x_0,\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
The sampling distribution of $y_0$ is:

$$
y_0 \sim 
N ( \beta_0+\beta_1x_0,\sigma^2)
$$
So the distribution of $y_0 -\hat{y_0}$ is:

$$
y_0 -\hat{y_0}
\sim
N \left ( 0,\sigma^2 \left ( 1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
And we take the steps to normalize the standard distribution:

$$
\frac {y_0 - \hat{y_0}}{\sigma \sqrt{ 1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim NID(0,1)
$$

We don't know the value of $\sigma$ so we use the "hatted" version and use a $t$ distribution:

$$
\frac {y_0 - \hat{y_0}}{\hat{\sigma} \sqrt{ 1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim t_{n-2}
$$

So we have the $(1-\alpha)100%$ Prediction Interval for $y_0(x_0)$ is:

$$
\hat{y_0} \pm t_{\alpha/2,n-2} 
\hat{\sigma} \sqrt{1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}
$$

Or written slightly differently using the relationship of $\hat{\sigma^2} = MS_{Res}$:
$$
\hat{y_0}  \pm t_{\alpha/2,n-2} 
\sqrt{MS_{Res} \left (1 +  \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right )}
$$

