---
title: Intro. to Linear Regression Analysis - Chapter 2 - Simple Linear Regression
author: Jeremy Buss
date: '2021-09-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---

### Simple Linear Regression

The characteristics of a **Simple Linear Regression** are as follows:

* One predictor $X$ that is known and constant
* One response variable $Y$
* Linear function $y=\beta_0 + \beta_1 x$

<br>


#### Simple Linear Regression - Population

The above is an idealized representation. Instead we have a set of n-pairs of data $(x_i, y_i)$ where $i=1, 2, ... n$. Additionally, the data will not perfectly fit the line. As a result we add an error term $\epsilon$. 

* Linear function  $Y_i=\beta_0 + \beta_1 X_i + \epsilon_i$ for $i=1,2,...n$
* Error term $\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)$
  * [$iid$](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables): identically, and independently distributed
  * $E(\epsilon_i)=0$
  * $Var(\epsilon_i)=\sigma^2$
* $Y_i$ is the $i$-th value of the response
* $X_i$ is the $i$-th value of the fixed value of the predictor
* $\beta_0, \beta_1,$ are model coefficients
* $\beta_0, \beta_1,$ and $\sigma^2$ are fixed unknown parameters of the model
* The mean response $\mu_{Y|X}=E(Y|X)$ has a straightline relationship with $X$ given by the population regression line
$$
\begin{align*}
\mu_{Y_i|X_i}=E(Y_i|X_i)&=E(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=\beta_0+\beta_1 X_i + E(\epsilon_i)\\
&=\beta_0+\beta_1 X_i
\end{align*}
$$
* The variance of $Y$ does not depend on $X$
$$
\begin{align*}
Var(Y_i|X_i) &= Var(\beta_0+\beta_1 X_i + \epsilon_i)\\
&=Var(\epsilon_i)
\end{align*}
$$

* Any fixed value of $X_i=x$, the response variable $Y_i$ varies with $N(\mu_{Y_i|X_i=x},\sigma^2)$
  * $Y_i|X_i = N(\beta_0 + \beta_1 X_i, \sigma^2)$


### Parameter Estimation and Model Fitting

Given that we have sample data $\{(x_1, y_1),(x_2, y_2),...(x_n,y_n)\}$ we wish to find the _best_ sample regression line.

We are interested in the values of $\beta_0$ and $\beta_1$ in the following _sample_ regression model:
$$
\begin{align*}
y_i &= f(x_i) + \epsilon_i\\
&= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}
$$

or using sample statistics $b_0$ and $b_1$ computed from the training data to estimate them: 
$$\hat{y_i} = b_0 + b_1 x_i$$
where $\hat{y_i}$ is a point estimate of $y_i$ with mean $\mu_{Y|X=x_i}=E(Y|X=x_i)$

#### Ordinary Least Squares (OLS)

We choose $b_0$ and $b_1$ that minimize the **sum of squared residuals** $SS_{res}$ or **residual sum of squares** $RSS$

* **residual** $e_i$ is a point estimate of $\epsilon_i$ and defined as the difference between our estimate of $y_i$ (i.e. $\hat{y_i}$) and the true value of $y_i$
  * $e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)$
* The sample regression line minimizes $SS_{res}$
$$
\begin{align*}
SS_{res}&=e_1^2 + e_2^2 + \cdots + e_n^2 = \sum_{i=1}^n e_i^2\\
&=(y_1 - b_0 + b_1 x_1)^2 + (y_2 - b_0 + b_1 x_2)^2 + \cdots + (y_n - b_0 + b_1 x_n)^2\\
&= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\\
\end{align*}
$$

#### Least Squares Estimate (LSE)

With the least squares estimate approach we choose $b_0$ and $b_1$ to minimize $SS_{res}$

$$
(b_0, b_1) = \text{arg} \; \underset {\beta_0, \beta_1}{\text{min}} \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2
$$
Taking the derivative and set them equal to 0 to arrive at the **normal equations**:

$$ 
\begin{align}
\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} &= -2\sum_{i=1}^n (y_i-b_0-b_1x_i)\\
\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} &= -2\sum_{i=1}^n x_i(y_i-b_0-b_1x_i)
\end{align}
$$
#### Least Squares Estimates: Solving for \beta_0 and \beta_1

* Solving for $\beta_0$ given $b_1$:
$$b_0 = \overline{y} - b_1 \overline {x}$$

* Solving for $\beta_1$ given that $b_0 = \overline{y} - b_1 \overline {x}$:
$$
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} = 
\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
=\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=
\frac{S_{xy}}{S_{xx}}
\end{align*}
$$

### Properties of Least Square Estimators

#### 1. $b_0$ and $b_1$ are linear combinations of $y_i$ 

for example for $b_1$ we use one of the forms:
$$
\begin{align*}
b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})}
\end{align*}
$$
To clean up the equation we use the term $c_i = \frac{x_i-\overline{x}}{S_{xx}}$ for $i=1,2,\dots n$ and the equation becomes simply a constant multiplied by $y_i$:
$$
\begin{align*}
b_1 = \sum_{i=1}^ny_i c_i
\end{align*}
$$

#### 2. $b_0$ and $b_1$ are unbiased estimators of $\beta_0$ and $\beta_1$

To show that $b_0$ is an unbiased estimator we want to show that $E(b_0)=\beta_0$. Let us simply start by substituting in the value for $b_0$ from the normal equations.

$$
E(b_0) = E(\overline{y} - b_1\overline{x})
$$
We use the helper function $\frac{1}{n}\sum_{i=1}^{n} {y_i}={\overline{y}}$ and pull out any values from the sum or Expected value function that are not dependent on them
$$
E(b_0) = \frac{1}{n}\sum_{i=1}^n E({y_i}) - E(b_1)\overline{x}
$$
Now substitute our equation for $y_i = \beta_0 + \beta_1 x_i$ and $E(b_1)=\beta_1$ in addition to the fact that $\beta_0, \beta_1, x_i$ are constant values the Expected Value of those individual items is simply the value of those items...
$$
E(b_0) = \frac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 x_i) - \beta_1 \overline{x}
$$
Again we use a helper function for $\sum_{i=1}^{n} {x_i}=n{\overline{x}}$
$$
E(b_0) = \frac{1}{n}(n\beta_0 + n\beta_1 \overline{x}) - \beta_1 \overline{x}
$$
Combining like terms we are left with simply:
$$
E(b_0) = \beta_0 
$$
