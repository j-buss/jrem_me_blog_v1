---
title: Transformations and Weighting to Correct Model Inadequacies
author: Jeremy Buss
date: '2021-10-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: false
katex: yes
---
```{r echo=FALSE}
library(blogdown)
```

## Model Assumptions

Regression model fitting has several implicit assumptions, including:

* $\epsilon \overset{iid}{\sim} NID(0,\sigma^2)$: **mean 0**, **constant variance**, **normally distributed**, and **uncorrelated** 
* The form of the model (**linearity**) and the specification of the predictors are correct.
* Check model adequacy by residual plots and lack-of-fit tests
* Methods when some assumptions are violated:
  * Data Transformation
  * Generalized Least Squares (GLS)
  * Weighted Least Squares (WLS)

## Data Transformation

### Variance Stabilization

The constant variance assumption is often violated when the response $y$ follows a distribution which is related to the mean.  
* Problems can be identified by examining the residual plots
* Variance stabilizing transformations can be selected empirically
  * If $\sigma^2$ is not constant, the coefficients will have larger standard errors (no longer BLUE)

```{r echo=FALSE, results='asis', escape=FALSE}
x <- data.frame("Relationship" = c("$\\sigma^2 \\propto \\text{constant}$",
                                   "$\\sigma^2 \\propto E(y)$",
                                   "$\\sigma^2 \\propto E(y)[1-E(y)]$",
                                   "$\\sigma^2 \\propto [E(y)]^2$",
                                   "$\\sigma^2 \\propto [E(y)]^3$",
                                   "$\\sigma^2 \\propto [E(y)]^4$"
                                   ),
                "Transformation" = c("$y^\\prime = y$ (no transformation)",
                                     "$y^\\prime = \\sqrt{y}$ (Square root; Poisson)",
                                     "$y^\\prime = \\arcsin{(\\sqrt{y})}$ (arcsin; binomial proportions $0 \\leq y_i \\leq 1$)",
                                     "$y^\\prime = \\ln{(y)}$ (natural log)",
                                     "$y^\\prime = y^{-1/2}$ (reciprocal square root)",
                                     "$y^\\prime = y^{-1}$ (reciprocal)"
                                     )
                )
knitr::kable(x=x,
             caption = "Useful Variance-Stabilizing Transformations",
             format="markdown", escape=FALSE, 
             col.names=c("Relationship of $\\sigma^2$ to $E(y)$","Transformation")
             )

```



### Linearization

* Nonlinearity may be detected by **lack-of-fit** test
* **Intrinsically Linear** refers to a nonlinear function which can be transformed into a linear one
* One should check the model assumptions on the transformed residuals


```{r echo=FALSE, results='asis', escape=FALSE}
x <- data.frame(
  
  "Col1" = c(
    "$y=\\beta_0 x^{\\beta_1}$",
    "$y=\\beta_0 e^{\\beta_1 x}$",
    "$y=\\beta_0 + \\beta_1 \\log {x}$",
    "$y=\\frac{x}{\\beta_0x - \\beta_1}$"
    ),
  
  "Col2" = c(
    "$y^\\prime = \\log {y}, x^\\prime = \\log {x}$",
    "$y^\\prime = \\ln{y}$",
    "$x^\\prime = \\log{x}$",
    "$y^\\prime = 1/y, x^\\prime = 1/x$"
  ),
    
  "Col3" = c(
    "$y^\\prime = \\log {\\beta_0} + \\beta_1 x^\\prime$",
    "$y^\\prime = \\ln {\\beta_0} + \\beta_1 x$",
    "$y^\\prime = \\beta_0 + \\beta_1 x^\\prime$",
    "$y^\\prime = \\beta_0 - \\beta_1 x^\\prime$"
  )
)
knitr::kable(x=x,
             caption = "Linearizable Functions and Corresponding Linear Form",
             format="markdown", escape=FALSE, 
             col.names=c("Linearizable Function",
                         "Transformation",
                         "Linear Form")
             )

```

* When these transformations are applied, the least squares estimator has least squares properties with the transformed data, not the original

## Methods for Selecting a Transformation

While many transformations may be selected empirically, the following methods can be used to select the appropriate transformation more formally.

### Box-Cox Method

* Goal: Transform $y$ to correct non-normality or non-constant variance
* **Power Transformation**: $y^{\lambda}$, where $\lambda$ is a parameter to be determined

[Box and Cox (1964) - Method](https://www.nuffield.ox.ac.uk/users/cox/cox72.pdf):
$$
y_i^{(\lambda)}=
\begin{cases}
   \frac{y_i^{\lambda}-1}{\lambda \dot{y}^{\lambda - 1}}, & \lambda \ne 0\\
   \dot{y} \ln{y_i}, & \lambda = 0
\end{cases}
$$

* Where $\dot{y} = \left ( \prod_{i=1}^n y_i \right )^{1/n}$ is the geometric mean of the observations
* The units of measurement do not change as $\lambda$ changes
* Models with different $\lambda$ are comparable

The model to be fit is:

$$
\mathbf{y}^{(\lambda)}=\mathbf{X\beta}+\mathbf{\epsilon}
$$

#### Computation Procedure

1. Create a grid of $\lambda$ values
2. For each value of $\lambda$ in the grid, regress $y^{(\lambda)}$ on the predictors, and obtain $SS_{Res}(\lambda)$
3. Take the $\lambda$ that leads to the smallest $SS_{Res}(\lambda)$

Once $\lambda$ is selected, we are free to fit the model using $y^{\lambda}$ as the response if $\lambda \ne 0$, and $\ln{y}$ if $\lambda = 0$

### Box and Tidwell

* Goal: the relationship between $y$ and the transformed regressors is approximately linear.
* $\epsilon \overset{iid}{\sim} NID(0,\sigma^2)$ is at least approximately satisfied
* Box and Tidwell (1962) proposed a procedure for estimating $\alpha_1, \alpha_2, \dots , \alpha_k$ in a model of the type

$$
y=\beta_0+\beta_1 z_1+\dots+\beta_k z_k + \epsilon
$$

where

$$
z_j=
\begin{cases}
   x_j^{\alpha_j}, & \alpha_j \ne 0\\
   \ln{(x_j)}, & \alpha_j = 0
\end{cases}
$$