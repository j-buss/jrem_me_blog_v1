---
title: Transformations and Weighting to Correct Model Inadequacies
author: Jeremy Buss
date: '2021-10-20'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - linear regression
draft: false
katex: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="model-assumptions" class="section level2">
<h2>Model Assumptions</h2>
<p>Regression model fitting has several implicit assumptions, including:</p>
<ul>
<li><span class="math inline">\(\epsilon \overset{iid}{\sim} NID(0,\sigma^2)\)</span>: <strong>mean 0</strong>, <strong>constant variance</strong>, <strong>normally distributed</strong>, and <strong>uncorrelated</strong></li>
<li>The form of the model (<strong>linearity</strong>) and the specification of the predictors are correct.</li>
<li>Check model adequacy by residual plots and lack-of-fit tests</li>
<li>Methods when some assumptions are violated:
<ul>
<li>Data Transformation</li>
<li>Generalized Least Squares (GLS)</li>
<li>Weighted Least Squares (WLS)</li>
</ul></li>
</ul>
</div>
<div id="data-transformation" class="section level2">
<h2>Data Transformation</h2>
<div id="variance-stabilization" class="section level3">
<h3>Variance Stabilization</h3>
<p>The constant variance assumption is often violated when the response <span class="math inline">\(y\)</span> follows a distribution which is related to the mean.</p>
<ul>
<li>Problems can be identified by examining the residual plots</li>
<li>Variance stabilizing transformations can be selected empirically
<ul>
<li>If <span class="math inline">\(\sigma^2\)</span> is not constant, the coefficients will have larger standard errors (no longer BLUE)</li>
</ul></li>
</ul>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>Useful Variance-Stabilizing Transformations</caption>
<colgroup>
<col width="30%" />
<col width="69%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Relationship of <span class="math inline">\(\sigma^2\)</span> to <span class="math inline">\(E(y)\)</span></th>
<th align="left">Transformation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2 \propto \text{constant}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = y\)</span> (no transformation)</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma^2 \propto E(y)\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \sqrt{y}\)</span> (Square root; Poisson)</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2 \propto E(y)[1-E(y)]\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \arcsin{(\sqrt{y})}\)</span> (arcsin; binomial proportions <span class="math inline">\(0 \leq y_i \leq 1\)</span>)</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma^2 \propto [E(y)]^2\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \ln{(y)}\)</span> (natural log)</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2 \propto [E(y)]^3\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = y^{-1/2}\)</span> (reciprocal square root)</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma^2 \propto [E(y)]^4\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = y^{-1}\)</span> (reciprocal)</td>
</tr>
</tbody>
</table>
</div>
<div id="linearization" class="section level3">
<h3>Linearization</h3>
<ul>
<li>Nonlinearity may be detected by <strong>lack-of-fit</strong> test</li>
<li><strong>Intrinsically Linear</strong> refers to a nonlinear function which can be transformed into a linear one</li>
<li>One should check the model assumptions on the transformed residuals</li>
</ul>
<table>
<caption><span id="tab:unnamed-chunk-3">Table 2: </span>Linearizable Functions and Corresponding Linear Form</caption>
<colgroup>
<col width="26%" />
<col width="34%" />
<col width="38%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Linearizable Function</th>
<th align="left">Transformation</th>
<th align="left">Linear Form</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y=\beta_0 x^{\beta_1}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \log {y}, x^\prime = \log {x}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \log {\beta_0} + \beta_1 x^\prime\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y=\beta_0 e^{\beta_1 x}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \ln{y}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \ln {\beta_0} + \beta_1 x\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(y=\beta_0 + \beta_1 \log {x}\)</span></td>
<td align="left"><span class="math inline">\(x^\prime = \log{x}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \beta_0 + \beta_1 x^\prime\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y=\frac{x}{\beta_0x - \beta_1}\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = 1/y, x^\prime = 1/x\)</span></td>
<td align="left"><span class="math inline">\(y^\prime = \beta_0 - \beta_1 x^\prime\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>When these transformations are applied, the least squares estimator has least squares properties with the transformed data, not the original</li>
</ul>
</div>
</div>
<div id="methods-for-selecting-a-transformation" class="section level2">
<h2>Methods for Selecting a Transformation</h2>
<p>While many transformations may be selected empirically, the following methods can be used to select the appropriate transformation more formally.</p>
<div id="box-cox-method" class="section level3">
<h3>Box-Cox Method</h3>
<ul>
<li>Goal: Transform <span class="math inline">\(y\)</span> to correct non-normality or non-constant variance</li>
<li><strong>Power Transformation</strong>: <span class="math inline">\(y^{\lambda}\)</span>, where <span class="math inline">\(\lambda\)</span> is a parameter to be determined</li>
</ul>
<p><a href="https://www.nuffield.ox.ac.uk/users/cox/cox72.pdf">Box and Cox (1964) - Method</a>:
<span class="math display">\[
y_i^{(\lambda)}=
\begin{cases}
   \frac{y_i^{\lambda}-1}{\lambda \dot{y}^{\lambda - 1}}, &amp; \lambda \ne 0\\
   \dot{y} \ln{y_i}, &amp; \lambda = 0
\end{cases}
\]</span></p>
<ul>
<li>Where <span class="math inline">\(\dot{y} = \left ( \prod_{i=1}^n y_i \right )^{1/n}\)</span> is the geometric mean of the observations</li>
<li>The units of measurement do not change as <span class="math inline">\(\lambda\)</span> changes</li>
<li>Models with different <span class="math inline">\(\lambda\)</span> are comparable</li>
</ul>
<p>The model to be fit is:</p>
<p><span class="math display">\[
\mathbf{y}^{(\lambda)}=\mathbf{X\beta}+\mathbf{\epsilon}
\]</span></p>
<div id="computation-procedure" class="section level4">
<h4>Computation Procedure</h4>
<ol style="list-style-type: decimal">
<li>Create a grid of <span class="math inline">\(\lambda\)</span> values</li>
<li>For each value of <span class="math inline">\(\lambda\)</span> in the grid, regress <span class="math inline">\(y^{(\lambda)}\)</span> on the predictors, and obtain <span class="math inline">\(SS_{Res}(\lambda)\)</span></li>
<li>Take the <span class="math inline">\(\lambda\)</span> that leads to the smallest <span class="math inline">\(SS_{Res}(\lambda)\)</span></li>
</ol>
<p>Once <span class="math inline">\(\lambda\)</span> is selected, we are free to fit the model using <span class="math inline">\(y^{\lambda}\)</span> as the response if <span class="math inline">\(\lambda \ne 0\)</span>, and <span class="math inline">\(\ln{y}\)</span> if <span class="math inline">\(\lambda = 0\)</span></p>
</div>
</div>
<div id="box-and-tidwell" class="section level3">
<h3>Box and Tidwell</h3>
<ul>
<li>Goal: the relationship between <span class="math inline">\(y\)</span> and the transformed regressors is approximately linear.</li>
<li><span class="math inline">\(\epsilon \overset{iid}{\sim} NID(0,\sigma^2)\)</span> is at least approximately satisfied</li>
<li>Box and Tidwell (1962) proposed a procedure for estimating <span class="math inline">\(\alpha_1, \alpha_2, \dots , \alpha_k\)</span> in a model of the type</li>
</ul>
<p><span class="math display">\[
y=\beta_0+\beta_1 z_1+\dots+\beta_k z_k + \epsilon
\]</span></p>
<p>where</p>
<p><span class="math display">\[
z_j=
\begin{cases}
   x_j^{\alpha_j}, &amp; \alpha_j \ne 0\\
   \ln{(x_j)}, &amp; \alpha_j = 0
\end{cases}
\]</span></p>
</div>
</div>
<div id="generalized-and-weighted-least-squares" class="section level2">
<h2>Generalized and Weighted Least Squares</h2>
<div id="generalized-least-squares-gls" class="section level3">
<h3>Generalized Least Squares (GLS)</h3>
<p>We can use a method called generalized least squares (GLS) for correlated errors with non-constant variance.</p>
<p>Originally we had:</p>
<ul>
<li><span class="math inline">\(\mathbf{y}=\mathbf{X\beta}+\mathbf{\epsilon}\)</span></li>
<li><span class="math inline">\(E(\mathbf{\epsilon}) =0\)</span></li>
<li><span class="math inline">\(Var(\mathbf{\epsilon}) = \sigma^2 \mathbf{I}\)</span></li>
</ul>
<p>Sometimes the assumption is unreasonable, and the error variance structure is <strong>known</strong>. Therefore we consider the model:</p>
<ul>
<li><span class="math inline">\(\mathbf{y}=\mathbf{X\beta}+\mathbf{\epsilon}\)</span></li>
<li><span class="math inline">\(E(\mathbf{\epsilon}) =0\)</span></li>
<li><span class="math inline">\(Var(\mathbf{\epsilon}) = \sigma^2 \mathbf{V}\)</span></li>
</ul>
<p>Where <span class="math inline">\(\mathbf{V}\)</span> is a <strong>known</strong> <span class="math inline">\(n \times n\)</span> matrix:</p>
<ul>
<li>If <span class="math inline">\(\mathbf{V}\)</span> is diagonal but with unequal diagonal elements: the observations <span class="math inline">\(\mathbf{y}\)</span> are <strong>uncorrelated</strong> but have <strong>unequal</strong> variances</li>
<li>If some of the off diagonal elements of <span class="math inline">\(\mathbf{V}\)</span> are nonzero: the observations <span class="math inline">\(\mathbf{y}\)</span> are <strong>correlated</strong></li>
</ul>
<p>Using our standard <span class="math inline">\(\mathbf{b} = ( \mathbf{X}^\intercal \mathbf{X}) ^ {-1} \mathbf{X}^\intercal \mathbf{y}\)</span> is not BLUE anymore:</p>
<ul>
<li><span class="math inline">\(\mathbf{b}\)</span> is still unbiased for <span class="math inline">\(\mathbf{\beta}\)</span>, i.e., <span class="math inline">\(E(\mathbf{b})=\mathbf{\beta}\)</span></li>
<li>However <span class="math inline">\(\mathbf{b}\)</span> does not have minimum variance. (not the “best”)</li>
</ul>
<p>GLS estimator:</p>
<p><span class="math display">\[
\mathbf{\hat{\beta}} = (\mathbf{X^\intercal V^{-1}X})^{-1}\mathbf{X^\intercal V^{-1}y}
\]</span></p>
<ul>
<li>The GLS estimator is BLUE</li>
<li><span class="math inline">\(Var(\mathbf{\hat{\beta}}) =\sigma^2 (\mathbf{X^\intercal} \mathbf{V}^{-1} \mathbf{X})^{-1}\)</span></li>
</ul>
<div id="anova-table-for-gls" class="section level4">
<h4>Anova Table for GLS</h4>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 3: </span>Analysis of Variance for Generalized Least Squares</caption>
<colgroup>
<col width="2%" />
<col width="82%" />
<col width="5%" />
<col width="5%" />
<col width="4%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="left">Sum of Squares</th>
<th align="left">Degrees of Freedom</th>
<th align="left">Mean Square</th>
<th align="left"><span class="math inline">\(F_0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left"><span class="math display">\[\begin{align*}
    SS_R&amp;=\mathbf{\hat{\beta}^\intercal}\mathbf{B^\intercal}\mathbf{z}\\
    &amp;=\mathbf{y^{\intercal}V}^{-1}\mathbf{X}(\mathbf{X^{\intercal}V}^{-1}\mathbf{X})^{-1}\mathbf{X^{\intercal}V}^{-1}\mathbf{y}
    \end{align*}
    \]</span></td>
<td align="left">k</td>
<td align="left"><span class="math inline">\(SS_R/k\)</span></td>
<td align="left"><span class="math inline">\(MS_R/MS_{Res}\)</span></td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left"><span class="math display">\[\begin{align*}
    SS_{Res}&amp;=\mathbf{z^{\intercal}z}-\mathbf{\hat{\beta}^{\intercal}B^{\intercal}z}\\
    &amp;=\mathbf{y}^{\intercal}\mathbf{V}^{-1}\mathbf{y}- \mathbf{y^{\intercal}V}^{-1}\mathbf{X}(\mathbf{X^{\intercal}V}^{-1}\mathbf{X})^{-1}\mathbf{X^{\intercal}V}^{-1}\mathbf{y}
    \end{align*}
    \]</span></td>
<td align="left">n-k-1</td>
<td align="left"><span class="math inline">\(SS_{Res}/(n-k-1)\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(\mathbf{z^{\intercal}z}=\mathbf{y}^{\intercal}\mathbf{V}^{-1}\mathbf{y}\)</span></td>
<td align="left">n-1</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\mathbf{y}\)</span> is scaled to have mean zero</p>
</div>
</div>
<div id="weighted-least-squares-wls" class="section level3">
<h3>Weighted Least Squares (WLS)</h3>
<ul>
<li>Weighted Least Squares is a special case of GLS that <span class="math inline">\(\mathbf{V}\)</span> has a special form.</li>
<li>When <span class="math inline">\(\mathbf{\epsilon}\)</span> are <em>uncorrelated</em> and have <em>unequal variances</em> the covariance matrix of <span class="math inline">\(\mathbf{\epsilon}\)</span> is</li>
</ul>
<p><span class="math display">\[
\sigma^2 \mathbf{V} = \sigma^2 
\begin{bmatrix}
\frac{1}{w_1} &amp; \space &amp; \space &amp; 0 \\
\space &amp; \frac{1}{w_2} &amp; \space \\
\space &amp; \space &amp; \space &amp; \ddots\\
0 &amp; \space &amp; \space &amp; \frac{1}{w_n}
\end{bmatrix}
\]</span></p>
<ul>
<li>Let $ = ^{-1}. Then <span class="math inline">\(\mathbf{W}\)</span> is diagonal with diagonal elements or <strong>weights</strong> <span class="math inline">\(w_1, w_2, \dots, w_n\)</span></li>
<li>The WLS esitmator <span class="math inline">\(\hat{\mathbf{\beta}} = (\mathbf{X}^{\intercal}\mathbf{WX})^{-1} \mathbf{X}^{\intercal}\mathbf{Wy}\)</span></li>
<li>Observations with large variances will have smaller weights</li>
</ul>
<p>The weighted least squares function:</p>
<p><span class="math display">\[
S(\beta_0, \beta_1) = \sum_{i=1}^n w_i (y_i - \beta_0 + \beta_1 x_i)^2
\]</span></p>
<p>The resulting least squares Normal Equations:</p>
<p><span class="math display">\[
\begin{align*}
\beta_0 \sum_{i=1}^n w_i + \beta_1 \sum_{i=1}^n w_i x_i &amp;= \sum_{i=1}^n w_i y_i\\
\beta_0 \sum_{i=1}^n w_i x_i + \beta_1 \sum_{i=1}^n w_i x_i^2 &amp;= \sum_{i=1}^n w_i y_i x_i
\end{align*}
\]</span></p>
<p>Which can be written as:</p>
<p><span class="math display">\[
\begin{align}
( \mathbf{X}^\intercal \mathbf{WX}) \mathbf{\beta} = \mathbf{X}^\intercal \mathbf{Wy}
\end{align}
\]</span></p>
<div id="methods-of-choosing-weights" class="section level4">
<h4>Methods of choosing weights</h4>
<ul>
<li>The weights, <span class="math inline">\(w_i\)</span>, or in general <span class="math inline">\(\mathbf{V}\)</span> must be <em>known</em></li>
<li>Prior information or knowledge</li>
<li>Residual Analysis:
<ul>
<li>Var(<span class="math inline">\(\epsilon\)</span>) <span class="math inline">\(\propto x_i\)</span>, suggests <span class="math inline">\(w_i = 1\x_i\)</span></li>
<li>When <span class="math inline">\(y_i\)</span> is an average of <span class="math inline">\(n_i\)</span> observations at <span class="math inline">\(x_i\)</span>, Var<span class="math inline">\((y_i)=\sigma^2/n_i\)</span> and suggest <span class="math inline">\(w_i = n_i\)</span></li>
</ul></li>
<li>Chosen inversely proportional to variances of measurement error</li>
</ul>
<p>If we have no idea of <span class="math inline">\(\mathbf{W}\)</span>, consider <strong>feasible GLS</strong> estimator:</p>
<p><span class="math display">\[
\mathbf{\hat{\beta}}_{FGLS} = ( \mathbf{X}^\intercal \mathbf{\hat{W}X})^{-1} \mathbf{X}^\intercal \mathbf{\hat{W}y}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\hat{W}}\)</span> is an estimate of <span class="math inline">\(\mathbf{W}\)</span> from some method.</p>
</div>
</div>
</div>
