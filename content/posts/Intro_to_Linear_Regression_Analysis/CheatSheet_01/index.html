---
title: "Linear Regression - Cheat Sheet"
author: "Jeremy Buss"
date: '2021-10-10'
output: pdf_document
categories:
- Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
- Applied Stats
- Regression Analysis
tags:
- R
- linear regression
draft: yes
katex: yes
slug: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><span class="math inline">\(y=\beta_0 + \beta_1 x\)</span></p>
<p><span class="math inline">\(e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)\)</span></p>
<p><span class="math inline">\(SS_{res} = \sum_{i=1}^n e_i^2= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2\)</span></p>
<p><span class="math inline">\(\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} = -2\sum_{i=1}^n(y_i-b_0-b_1x_i)\)</span></p>
<p><span class="math inline">\(\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} = -2\sum_{i=1}^nx_i(y_i-b_0-b_1x_i)\)</span></p>
<p><span class="math inline">\(b_0 = \overline{y} - b_1 \overline {x}\)</span></p>
<p><span class="math inline">\(b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} =\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=\frac{S_{xy}}{S_{xx}}\)</span></p>
<p><span class="math inline">\(Var(b_1) = \sigma^2 \sum_{i=1}^n c_i^2 = \frac {\sigma^2 \sum_{i=1}^n (x_i-\overline {x})^2}{S_{xx}^2} = \frac{\sigma^2}{S_{xx}}\)</span></p>
<p><span class="math inline">\(Var(b_0) = \sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \right )\)</span></p>
<p><span class="math inline">\(E(b_0) = \beta_0; E(b_1)=\beta_1\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^n(y_i - \hat {y_i}) = \sum_{i=1}^n e_i = 0\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^ny_i = \sum_{i=1}^n \hat{y}\)</span></p>
<p>LRM contains centroid <span class="math inline">\((\overline{x},\overline{y})\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^n e_i x_i = 0\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^n e_i \hat{y_i} = 0\)</span></p>
<p><span class="math inline">\(\hat{\sigma}^2 = \frac {SS_{Res}}{n-2}=MS_{Res}\)</span></p>
<p><span class="math inline">\(\beta_1\)</span>: <span class="math inline">\(b_1 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2/S_{xx}}\)</span></p>
<p><span class="math inline">\(\beta_0\)</span>: <span class="math inline">\(b_0 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}\)</span></p>
<p><span class="math inline">\(\hat{\sigma^2} = MS_{res} \sim \sigma^2 \frac {\chi_{n-2}^2}{n-2}\)</span></p>
<p><span class="math inline">\(\hat{\sigma^2} \space \text{range} \space (\frac{SS_{Res}}{\chi_{\alpha/2,n-2}^2},\frac{SS_{Res}}{\chi_{1-\alpha/2,n-2}^2})\)</span></p>
<p><span class="math inline">\(se(\hat{\beta_1}) = \sqrt{\frac{MS_{Res}}{S_{xx}}}\)</span></p>
<p><span class="math inline">\(se(\hat{\beta_0}) = \sqrt{MS_{Res} (1/n + \overline{x}^2/S_{xx})}\)</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span></p>
<p>if Critical Value Method: <span class="math inline">\(\vert t_{test} \vert &gt; t_{\alpha/2,n-2}\)</span></p>
<p>P-Value Method: p-value = <span class="math inline">\(2P(t_{n-2} &gt; \vert t_{test} \vert ) &lt; \alpha\)</span></p>
<p>Test statistic: <span class="math inline">\(t_{test} = \frac {b_1 - \beta_1^0}{se(b_1)} \sim t_{n-2}\)</span> under <span class="math inline">\(H_0\)</span></p>
<p>Test statistic: <span class="math inline">\(t_{test} = \frac {b_0 - \beta_0^0}{se(b_0)} \sim t_{n-2}\)</span> under <span class="math inline">\(H_0\)</span></p>
<p><span class="math inline">\((SS_T)\)</span> : <span class="math inline">\(\sum_{i=1}^n (y_i - \overline{y})^2\)</span></p>
<p><span class="math inline">\((SS_R)\)</span>: <span class="math inline">\(\sum_{i=1}^n (\hat{y_i} - \overline{y})^2\)</span> or <span class="math inline">\(\hat{\beta_1}S_{xy}\)</span></p>
<p><span class="math inline">\((SS_{Res})\)</span>: <span class="math inline">\(\sum_{i=1}^n (y_i - \hat{y_i})^2\)</span> or <span class="math inline">\(SS_T - \hat{\beta_1}S_{xy}\)</span></p>
<p><span class="math inline">\(F_{test}=\frac {SS_R/df_R}{SS_{Res}/df_{Res}}=\frac{SS_R/1}{SS_{Res}/{(n-2)}}=\frac{MS_R}{MS_{Res}}\)</span></p>
<p>reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F_{test} &gt; F_{\alpha,1,n-2}\)</span> or p-value = <span class="math inline">\(P(F_{1,n-2} &gt; F_{test}) &gt; \alpha\)</span></p>
<p><span class="math inline">\(R^2 = \frac {SS_R}{SS_T}= \frac {SS_T - SS_{Res}}{SS_T} = 1-\frac {SS_{Res}}{SS_T}\)</span></p>
<p><span class="math inline">\(\hat{\mu_{y|x_0}} \pm t_{\alpha/2,n-2} \hat{\sigma} \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}\)</span></p>
<p><span class="math inline">\(\hat{y_0} \pm t_{\alpha/2,n-2} \hat{\sigma} \sqrt{1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}\)</span></p>
<p><span class="math inline">\(f(y_i|\beta_0,\beta_1,\sigma^2) = \frac{e^{\frac{-(y_i-\beta_0-\beta_i x_i)^2}{2\sigma^2}}}{\sqrt{2 \pi \sigma^2}}\)</span></p>
<p><span class="math inline">\(L(\beta_0,\beta_1,\sigma^2|Y_i) = \prod_{i=1}^n e^{\frac{-(y_i-\beta_0-\beta_i x_i)^2}{2\sigma^2}}\left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^n\)</span></p>
<p><span class="math inline">\(l(\beta_0,\beta_1,\sigma^2|Y_i) = \frac{-1}{2\sigma^2}\sum_{i=1}^n(y_i-\beta_0-\beta_i x_i)^2 - \frac {n}{2} ln(2 \pi ) - \frac {n}{2} ln(\sigma^2 )\)</span></p>
<p><span class="math inline">\(\frac{\partial l} {\partial {\beta_0}} \bigg\vert_{\hat{\beta_0},\hat{\beta_1},\hat{\sigma}} = \frac{1}{\hat{\sigma}^2}\sum_{i=1}^n (y_i-\hat{\beta_0}-\hat{\beta}_1x_i) = 0\)</span></p>
<p><span class="math inline">\(\frac{\partial l}{\partial {\beta_1}} \bigg\vert_{\hat{\beta_0},\hat{\beta_1},\hat{\sigma}} = \frac{1}{\hat{\sigma}^2} \sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}_1x_i)x_i = 0\)</span></p>
<p><span class="math inline">\(\frac{\partial l}{\partial {\sigma^2}} \bigg\vert_{\hat{\beta_0},\hat{\beta_1},\hat{\sigma}} = -\frac{1}{2\hat{\sigma}^2} + \frac{1}{2\hat{\sigma}^4} \sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2 = 0\)</span></p>
<p><span class="math inline">\(\hat{\beta_0} = \overline{y} - \hat{\beta_1} \overline {x}=b_0\)</span></p>
<p><span class="math inline">\(\hat{\beta_1} =\frac{\sum_{i=1}^n(x_i - \overline{x})y_i}{\sum_{i=1}^n(x_i -\overline{x})^2} = b_1\)</span></p>
<p><span class="math inline">\(\hat{\sigma^2} =\frac{\sum_{i=1}^n(y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2}{n}\)</span></p>
<p><span class="math inline">\(Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \epsilon_i\)</span></p>
<p><span class="math inline">\(y_i= \beta_0 + \sum_{j=1}^k\beta_j x_{ij} + \epsilon_i,\quad i=1,2,\dots,n\)</span></p>
<p><span class="math inline">\(S(\beta_0, \beta_1,\dots,\beta_k) = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left ( y_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij} \right )^2\)</span></p>
<p><span class="math inline">\(\frac{\partial S}{\partial {\beta_0}} \bigg\vert_{b_0,b_1,\dots,b_k} = -2\sum_{i=1}^n ( y_i - b_0 -\sum_{j=1}^k b_j x_{ij} ) = 0\)</span></p>
<p><span class="math inline">\(\frac{\partial S}{\partial {\beta_j}} \bigg\vert_{b_0,b_1,\dots,b_k} = -2\sum_{i=1}^n \left ( y_i-b_0-\sum_{j=1}^k b_j x_{ij} \right ) x_{ij} = 0\)</span></p>
<p><span class="math inline">\(\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon} \space, \mathbf{y} = \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\mathbf{X} = \begin{bmatrix}1 &amp; x_{11} &amp; x_{12} &amp; \dots &amp; x_{1k}\\1 &amp; x_{21} &amp; x_{22} &amp; \dots &amp;x_{2k}\\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\1 &amp; x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{nk}\\\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\mathbf{\beta} = \begin{bmatrix} \beta_0\\ \beta_1\\ \vdots\\ \beta_k \end{bmatrix}, \space \mathbf{\epsilon} = \begin{bmatrix} \epsilon_1\\ \epsilon_2\\ \vdots\\ \epsilon_n \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\mathbf{X}\)</span>: Design matrix <span class="math inline">\(\mathbf{\epsilon} \sim N(\mathbf{0},\sigma^2 \mathbf{I})\)</span></p>
<p><span class="math inline">\(S(\mathbf{\beta}) =\mathbf{y}^\intercal\mathbf{y}-2\mathbf{\beta^\intercal}\mathbf{X}^\intercal\mathbf{y}+\mathbf{\beta^\intercal}\mathbf{X^\intercal}\mathbf{X}\mathbf{\beta}\)</span></p>
<p><span class="math inline">\(\frac{\partial{\mathbf{t}^\intercal} \mathbf{a}}{\partial{t}} = \frac{\partial{\mathbf{a}^\intercal} \mathbf{t}}{\partial{t}} = \mathbf{a}\)</span></p>
<p><span class="math inline">\(\frac{\partial{\mathbf{t}^\intercal} \mathbf{A} \mathbf{t}}{\partial{t}} = 2 \mathbf{A}\mathbf{t}\)</span></p>
<p><span class="math inline">\(\frac{\partial{S}}{\partial\beta}\bigg\vert_{\mathbf{b}} = -2 \mathbf{X^\intercal} \mathbf{y} + 2 \mathbf{X^\intercal} \mathbf{X} \mathbf{b} = 0\)</span></p>
<p><span class="math inline">\(\mathbf{b} = ( \mathbf{X}^\intercal \mathbf{X}) ^ {-1} \mathbf{X}^\intercal \mathbf{y}\)</span></p>
<p><span class="math inline">\(\mathbf{\hat{y}} = \mathbf{X} \mathbf{b} = \mathbf{X} ( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal} \mathbf{y} = \mathbf{H} \mathbf{y}\)</span></p>
<p><strong>hat matrix</strong> <span class="math inline">\(\mathbf{X} ( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal}\)</span></p>
<p><span class="math inline">\(\mathbf{e} = \mathbf{y}-\mathbf{\hat{y}} = \mathbf{y} - \mathbf{Xb} = \mathbf{y}-\mathbf{Hy}=(\mathbf{I}-\mathbf{H})\mathbf{y}\)</span></p>
<p><span class="math inline">\(\mathbf{H}\)</span>, <span class="math inline">\(\mathbf{I-H}\)</span> <strong>symmetric</strong> <strong>idempotent</strong> <strong>projection</strong> matrices</p>
<p><span class="math inline">\(\mathbf{H}\)</span> projects <span class="math inline">\(\mathbf{y}\)</span> to <span class="math inline">\(\mathbf{\hat{y}}\)</span> on column space <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(Col(\mathbf{X})\)</span></p>
<p><span class="math inline">\(\mathbf{I-H}\)</span> projects <span class="math inline">\(\mathbf{y}\)</span> to <span class="math inline">\(\mathbf{e}\)</span> on space <strong>perpendicular</strong> to <span class="math inline">\(Col(\mathbf{x})\)</span></p>
<p><span class="math inline">\(Col(\mathbf{X}) = \{ \mathbf{X}\mathbf{b}: \mathbf{b} \in \mathbf{R}^p \}\)</span></p>
<p><span class="math inline">\(\mathbf{y} \notin Col(\mathbf{X})\)</span></p>
<p><span class="math inline">\(\hat{\mathbf{y}} = \mathbf{Xb} = \mathbf{Hy} \in Col(\mathbf{X})\)</span></p>
<p>Minimize distance of <span class="math inline">\(\textcolor{red}{A}\)</span> to <span class="math inline">\(Col(\mathbf{X})\)</span>: Find the point in <span class="math inline">\(Col(\mathbf{X})\)</span> that is closest to <span class="math inline">\(\textcolor{red}{A}\)</span></p>
<p><span class="math inline">\(\mathbf{e} = \mathbf{y}-\mathbf{\hat{y}} = \mathbf{y} - \mathbf{Xb} = \mathbf{y}-\mathbf{Hy}=(\mathbf{I}-\mathbf{H})\mathbf{y} \bot Col(\mathbf{X})\)</span></p>
<p><span class="math inline">\(\mathbf{X^\intercal} (\mathbf{y} - \mathbf{Xb})=0\)</span></p>
<p><span class="math inline">\(Var(\mathbf{b})=\sigma^2(\mathbf{X}^\intercal \mathbf{X})^{-1}\)</span></p>
<p><span class="math inline">\(SS_{Res} = \mathbf{y^\intercal y} - \mathbf{b^\intercal X^\intercal y}\)</span></p>
<p><span class="math inline">\(MS_{Res} = \frac{SS_{Res}}{n-p} \space \space \text{with} \space p=k+1\)</span></p>
<p><span class="math inline">\(\hat{\sigma^2}=MS_{Res}\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>, i.e.Â <span class="math inline">\(E[MS_{Res}] = \sigma^2\)</span></p>
<p><span class="math inline">\(\hat{\sigma^2}\)</span> measures <strong>unexplained</strong> var. prefer small residual mean square.</p>
<p><span class="math inline">\(H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0\)</span></p>
<p><span class="math inline">\(H_1: \beta_j \ne 0\)</span> for at least one <span class="math inline">\(j\)</span></p>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>ANOVA Table</caption>
<thead>
<tr class="header">
<th align="left">Source of Variation</th>
<th align="left">Sum of Squares</th>
<th align="left">Degrees of Freedom</th>
<th align="left">Mean Square</th>
<th align="left"><span class="math inline">\(F_{test}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left"><span class="math inline">\(SS_R\)</span></td>
<td align="left">k</td>
<td align="left"><span class="math inline">\(MS_R\)</span></td>
<td align="left"><span class="math inline">\(MS_R/MS_{Res}\)</span></td>
</tr>
<tr class="even">
<td align="left">Residual</td>
<td align="left"><span class="math inline">\(SS_{Res}\)</span></td>
<td align="left">n-k-1</td>
<td align="left"><span class="math inline">\(MS_{Res}\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(SS_T\)</span></td>
<td align="left">n-1</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<ul>
<li><em>Note: k is # of coefficients for regressors. For SLR k = 1</em></li>
</ul>
<p><span class="math inline">\(SS_T = \mathbf{y^\intercal y} - \frac{1}{n} \sum_{i=1}^n y_i^2\)</span></p>
<p><span class="math inline">\(SS_{Res} = \mathbf{y^\intercal y} - \mathbf{b^\intercal X^\intercal y}\)</span></p>
<p><span class="math inline">\(SS_R = SS_T - SS_{Res} = \mathbf{b^\intercal X^\intercal y} - \frac{1}{n} \sum_{i=1}^n y_i^2\)</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F_{test} &gt; F_{\alpha,k,n-k-1}\)</span></p>
<p><span class="math inline">\(E[MS_{Res}] = \sigma^2\)</span></p>
<p><span class="math inline">\(E[MS_R] = \sigma^2 + \frac{\beta_{1:k}^\intercal \mathbf{X_c ^\intercal X_c \beta_{1:k}}}{k\sigma^2}\)</span> where <span class="math inline">\(\beta_{1:k}=(\beta_1,\dots,\beta_k)^\intercal\)</span></p>
<p><span class="math inline">\(\mathbf{X_c} = \begin{bmatrix} x_{11} - \overline{x_1} &amp; x_{12} - \overline{x_2} &amp; \dots &amp; x_{1k}- \overline{x_k}\\ x_{21} - \overline{x_1} &amp; x_{22} - \overline{x_2} &amp; \dots &amp; x_{2k}- \overline{x_k}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ x_{n1} - \overline{x_1} &amp; x_{n2} - \overline{x_2} &amp; \dots &amp; x_{nk}- \overline{x_k}\\ \end{bmatrix}\)</span></p>
<p><span class="math inline">\(R^2 = \frac {SS_R}{SS_T}= \frac {SS_T - SS_{Res}}{SS_T} = 1-\frac {SS_{Res}}{SS_T}\)</span></p>
<p><span class="math inline">\(R_{adj}^2 = 1-\frac {SS_{Res}/(n-p)}{SS_T/(n-1)}\)</span></p>
<p>Penalty (through p) for the number of variables in model</p>
<p>Partial: Tests the contribution of <span class="math inline">\(X_j\)</span> given all other regressors in the model</p>
<p><span class="math inline">\(H_0:\beta_j = 0\)</span> and <span class="math inline">\(H_1: \beta_j \ne 0\)</span></p>
<p><span class="math inline">\(t_{test} = \frac{b_j}{\sqrt{\hat\sigma^2 C_{jj}}}\)</span>, where <span class="math inline">\(C_{jj}\)</span> is the <span class="math inline">\(j\)</span>-th diagonal element of <span class="math inline">\((\mathbf{X^\intercal X})^{-1}\)</span></p>
<p><span class="math inline">\(E(a+cY) = a+cE(Y)\)</span></p>
<p><span class="math inline">\(Var(a+cY) = c^2 Var(Y)\)</span></p>
<p><span class="math inline">\(Cov(X,Y)=E[(X-E(X))(Y-E(Y))]\)</span></p>
<p><span class="math inline">\(Cov(a+cX, b+dY)=cdCov(X,Y)\)</span></p>
<p><span class="math inline">\(E(\sum_{i=1}^n a_i Y_i) \sum_{i=1}^n a_i E(Y_i)\)</span></p>
<p><span class="math inline">\(Var(\sum_{i=1}^n a_i Y_i) = \sum_{i=1}^n \sum_{j=1}^n a_i a_j Cov(Y_i, Y_j)\)</span></p>
<p>If <span class="math inline">\(Y_1, Y_2,...\)</span> inde. <span class="math inline">\(Cov(Y_i,Y_j)=0\)</span> for <span class="math inline">\(i \ne j\)</span> and <span class="math inline">\(Var(\sum_{i=1}^n a_i Y_i) = \sum_{i=1}^n a_i^2 Var(Y_i)\)</span></p>
<p><span class="math inline">\(Y\sim N(\mu, \sigma^2), Z=\frac{Y-\mu}{\sigma}\sim N(0,1)\)</span></p>
<p><span class="math inline">\(Y_i \overset{iid}{\sim} N(\mu_i, \sigma_i^2)\)</span>, then distr. <span class="math inline">\(N(\sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2, \sigma_i^2)\)</span></p>
<p>If <span class="math inline">\(Z\sim N(0,1)\)</span> then <span class="math inline">\(Z^2 \sim \chi_1^2\)</span></p>
<p>If <span class="math inline">\(Z_i \overset {iid}{\sim} N(0,1)\)</span> then <span class="math inline">\(\sum_{i=1}^n Z^2 \sim \chi_n^2\)</span></p>
<p>rank = # of Linearly Independent columns</p>
<p>Idempotent <span class="math inline">\(AA=A\)</span></p>
<p>Orthogonal: <span class="math inline">\(A^{-1}=A^\intercal\)</span> and <span class="math inline">\(A^{\intercal}A=I\)</span></p>
<p>Symmetric: <span class="math inline">\(A = A^\intercal\)</span></p>
<p>Inverse: <span class="math inline">\(A^{-1}A = AA^{-1}=I\)</span></p>
<p>Quadratic: <span class="math inline">\(y^\intercal A y = \sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j\)</span></p>
<p>trace: sum of diags</p>
<p><span class="math inline">\(\mathbf{A} \mathbf{x} = \lambda \mathbf{x}\)</span></p>
<p><span class="math inline">\(E(\mathbf{By})=\mathbf{B}E(\mathbf{y})\)</span></p>
<p><span class="math inline">\(Var(\mathbf{By})=\mathbf{B}Var(\mathbf{y})\mathbf{B^\intercal} = \mathbf{B \Sigma B^\intercal}\)</span></p>
<p><span class="math inline">\(\mathbf{y} \sim N_n(\mathbf{\mu},\sigma^2 \mathbf{I})\)</span> and <span class="math inline">\(\overline{y} = \frac{1}{n} \mathbf{1^\intercal Y}\)</span></p>
<p><span class="math inline">\(\overline{y} \sim N(\frac{1}{n}\mathbf{1^\intercal Y},\frac{1}{n}\mathbf{1^\intercal}(\sigma^2 \mathbf{I})\frac{1}{n}\mathbf{1})=N(\mu,\frac{\sigma^2}{n})\)</span></p>
