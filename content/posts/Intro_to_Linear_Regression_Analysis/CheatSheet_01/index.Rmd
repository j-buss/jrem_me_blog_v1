---
title: "Linear Regression - Cheat Sheet"
author: "Jeremy Buss"
date: '2021-10-10'
output: pdf_document
categories:
- Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
- Applied Stats
- Regression Analysis
tags:
- R
- linear regression
draft: yes
katex: yes
slug: []
---
```{r echo=FALSE}
library(blogdown)
```
$y=\beta_0 + \beta_1 x$

$e_i = y_i - \hat{y_i} = y_i - (b_0 + b_i x_i)$

$SS_{res} = \sum_{i=1}^n e_i^2= \sum_{i=1}^n (y_i - b_0 + b_1 x_i)^2$

$\frac{\partial SS_{res}}{\partial {\beta_0}} \bigg\vert_{b_0,b_1} = -2\sum_{i=1}^n(y_i-b_0-b_1x_i)$

$\frac{\partial SS_{res}}{\partial {\beta_1}} \bigg\vert_{b_0,b_1} = -2\sum_{i=1}^nx_i(y_i-b_0-b_1x_i)$

$b_0 = \overline{y} - b_1 \overline {x}$

$b_1 = \frac{\sum_{i=1}^ny_i(x_i - \overline{x})}{\sum_{i=1}^nx_i(x_i -\overline{x})} = \frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})(x_i -\overline{x})} =\frac{\sum_{i=1}^n(x_i - \overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2}=\frac{S_{xy}}{S_{xx}}$

$Var(b_1) = \sigma^2 \sum_{i=1}^n c_i^2 = \frac {\sigma^2 \sum_{i=1}^n (x_i-\overline {x})^2}{S_{xx}^2} = \frac{\sigma^2}{S_{xx}}$

$Var(b_0) = \sigma^2 \left ( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \right )$

$E(b_0) = \beta_0; E(b_1)=\beta_1$

$\sum_{i=1}^n(y_i - \hat {y_i}) = \sum_{i=1}^n e_i = 0$

$\sum_{i=1}^ny_i = \sum_{i=1}^n \hat{y}$

Least Squares Regression line passes through the centroid point $(\overline{x},\overline{y})$

$\sum_{i=1}^n e_i x_i = 0$

$\sum_{i=1}^n e_i \hat{y_i} = 0$

$\hat{\sigma}^2 = \frac {SS_{Res}}{n-2}=MS_{Res}$

$\beta_1$: $b_1 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2/S_{xx}}$

$\beta_0$: $b_0 \pm t_{\alpha/2,n-2} \sqrt{\hat{\sigma}^2 (1/n + \overline{x}^2/S_{xx})}$

$\hat{\sigma^2} = MS_{res} \sim \sigma^2 \frac {\chi_{n-2}^2}{n-2}$

$\hat{\sigma^2} \space \text{range} \space  (\frac{SS_{Res}}{\chi_{\alpha/2,n-2}^2},\frac{SS_{Res}}{\chi_{1-\alpha/2,n-2}^2})$

$se(\hat{\beta_1}) = \sqrt{\frac{MS_{Res}}{S_{xx}}}$

$se(\hat{\beta_0}) = \sqrt{MS_{Res} (1/n + \overline{x}^2/S_{xx})}$

Reject $H_0$ in favor of $H_1$ if Critical Value Method: $\vert t_{test} \vert > t_{\alpha/2,n-2}$
P-Value Method: p-value = $2P(t_{n-2} > \vert t_{test} \vert ) < \alpha$

Test statistic: $t_{test} = \frac {b_1 - \beta_1^0}{se(b_1)} \sim t_{n-2}$ under $H_0$

Test statistic: $t_{test} = \frac {b_0 - \beta_0^0}{se(b_0)} \sim t_{n-2}$ under $H_0$


$(SS_T)$ : $\sum_{i=1}^n (y_i - \overline{y})^2$

$(SS_R)$: $\sum_{i=1}^n (\hat{y_i} - \overline{y})^2$ or $\hat{\beta_1}S_{xy}$

$(SS_{Res})$: $\sum_{i=1}^n (y_i - \hat{y_i})^2$ or $SS_T - \hat{\beta_1}S_{xy}$

$F_{test}=\frac {SS_R/df_R}{SS_{Res}/df_{Res}}=\frac{SS_R/1}{SS_{Res}/{(n-2)}}=\frac{MS_R}{MS_{Res}}$

Therefore, to test the hypothesis $H_0: \beta_1 = 0$ compute $F_{test}$ and reject $H_0$ if 

* $F_{test} > F_{\alpha,1,n-2}$
* p-value = $P(F_{1,n-2} > F_{test}) > \alpha$

Combining all these definitions we have...


```{r echo=FALSE, results='asis', escape=FALSE}
x <- data.frame("Source of Variation" = c("Regression","Residual","Total"),
                "Sum of Squares" = c("$SS_R$","$SS_{Res}$","$SS_T$"),
                "Degrees of Freedom" = c("1", "n-2", "n-1"),
                "Mean Square" = c("$MS_R$", "$MS_{Res}$",""),
                "$F_0$" = c("$MS_R/MS_{Res}$", "","")
                )
knitr::kable(x=x,caption = "ANOVA Table",format="markdown", escape=FALSE, col.names=c("Source of Variation","Sum of Squares","Degrees of Freedom","Mean Square","$F_{test}$"))

```

In SLR, the $F$-test of ANOVA give the same result as a two-sided $t$-test of $H_0$:$\beta_1$=0

### Coefficient of Determination

The **coefficient of determination** is defined as the following:

$$
R^2 = \frac {SS_R}{SS_T}= \frac {SS_T - SS_{Res}}{SS_T} = 1-\frac {SS_{Res}}{SS_T}
$$

Since $SS_T$ is the measure of variability in y without considering the effect of the regressor variable $x$ and $SS_{Res}$ is a measure of the variability in $y$ remaining after $x$ has been considered.

$R^2$ is often called the proportion of variation explained by the regressor $x$.

### Prediction

#### Mean Response 

If $x_0$ is within the range of $x$, an unbiased point estimate of $E(y|x_0)$ is:

$$
\widehat{E(y|x_0)}=\hat{\mu_{y|x_0}}=b_0+b_1x_0
$$

The variance of $\hat{\mu_{y|x_0}}$ is:
$$
\begin{align*}
Var(\hat{\mu_{y|x_0}}) &= Var(b_0+b_1 x_0) = Var(\overline{y} + b_1(x_0-\overline{x}))\\
&=\frac{\sigma^2}{n}+ \frac{\sigma^2 (x_0 - \overline{x})^2}{S_{xx}}=\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right )
\end{align*}
$$

So the sampling distribution of $\hat{\mu_{y|x_0}}$ is:

$$
N \left ( \beta_0+\beta_1x_0,\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
So now we have
$$
(\widehat{\mu_{y|x_0}}=b_0+b_1 x_0)
\sim
N \left ( \beta_0+\beta_1 x_0,\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
And we take the steps to normalize the standard distribution:

$$
\frac {(b_0+b_1 x_0)-(\beta_0+\beta_1 x_0)}{\sigma \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim NID(0,1)
$$

We don't know the value of $\sigma$ so we use the "hatted" version and use a $t$ distribution:

$$
\frac {(b_0+b_1 x_0)-(\beta_0+\beta_1 x_0)}{\hat{\sigma} \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim t_{n-2}
$$

So we have the $(1-\alpha)100%$ Confidence interval for $E(y|x_0)$ is:

$$
\hat{\mu_{y|x_0}} \pm t_{\alpha/2,n-2} 
\hat{\sigma} \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}
$$

Or written slightly differently using the relationship of $\hat{\sigma^2} = MS_{Res}$:
$$
\hat{\mu_{y|x_0}} \pm t_{\alpha/2,n-2} 
\sqrt{MS_{Res} \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right )}
$$

#### Point Estimate

The sampling distribution of $\hat{y_0}$ is:

$$
\hat{y_0}=b_0+b_1 x_0 \sim 
N \left ( \beta_0+\beta_1x_0,\sigma^2 \left ( \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
The sampling distribution of $y_0$ is:

$$
y_0 \sim 
N ( \beta_0+\beta_1x_0,\sigma^2)
$$
So the distribution of $y_0 -\hat{y_0}$ is:

$$
y_0 -\hat{y_0}
\sim
N \left ( 0,\sigma^2 \left ( 1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right ) \right )
$$
And we take the steps to normalize the standard distribution:

$$
\frac {y_0 - \hat{y_0}}{\sigma \sqrt{ 1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim NID(0,1)
$$

We don't know the value of $\sigma$ so we use the "hatted" version and use a $t$ distribution:

$$
\frac {y_0 - \hat{y_0}}{\hat{\sigma} \sqrt{ 1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}} \sim t_{n-2}
$$

So we have the $(1-\alpha)100%$ Prediction Interval for $y_0(x_0)$ is:

$$
\hat{y_0} \pm t_{\alpha/2,n-2} 
\hat{\sigma} \sqrt{1 + \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}}}
$$

Or written slightly differently using the relationship of $\hat{\sigma^2} = MS_{Res}$:
$$
\hat{y_0}  \pm t_{\alpha/2,n-2} 
\sqrt{MS_{Res} \left (1 +  \frac{1}{n}+\frac{(x_0-\overline{x})^2}{S_{xx}} \right )}
$$

### Maximum Likelihood Estimation

We can also use the **method of maximum likelihood** to derive the linear regression estimators.

Let's start with what we know:

* $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
* $\epsilon \overset{iid}{\sim} NID(0,\sigma^2)$ 

This means that the sampling distribution of $Y_i$:
$$
Y_i \overset{iid}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)
$$
Now let's write the probability density function (pdf) for the distribution of $Y_i$:
$$
f(y_i|\beta_0,\beta_1,\sigma^2) = \frac{e^{\frac
{-(y_i-\beta_0-\beta_i x_i)^2}
{2\sigma^2}}}
{\sqrt{2 \pi \sigma^2}}
$$

Well, that is one function. Now to get the distribution for all n we take the product over all n:

$$
\begin{align*}
L(\beta_0,\beta_1,\sigma^2|Y_i) &= \prod_{i=1}^n f(y_i|\beta_0,\beta_1,\sigma^2)\\
&= \prod_{i=1}^n 
e^{\frac
{-(y_i-\beta_0-\beta_i x_i)^2}
{2\sigma^2}}
\left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^n
\end{align*}
$$

Now we use the product rule for exponents:

$$
\begin{align*}
L(\beta_0,\beta_1,\sigma^2|Y_i) &= 
&=  
\left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^n
e^{\frac{-1}{2\sigma^2}
\sum_{i=1}^n(y_i-\beta_0-\beta_i x_i)^2}
\end{align*}
$$

Now we take the natural log to get the log likelihood:

$$
\begin{align*}
l(\beta_0,\beta_1,\sigma^2|Y_i) &= 
\frac{-1}{2\sigma^2}
\sum_{i=1}^n(y_i-\beta_0-\beta_i x_i)^2 - \frac {n}{2} ln(2 \pi \sigma^2)\\
&= \frac{-1}{2\sigma^2}
\sum_{i=1}^n(y_i-\beta_0-\beta_i x_i)^2 - \frac {n}{2} ln(2 \pi \sigma^2)\\
&= \frac{-1}{2\sigma^2}
\sum_{i=1}^n(y_i-\beta_0-\beta_i x_i)^2 - \frac {n}{2} ln(2 \pi ) - \frac {n}{2} ln(\sigma^2 )\\
\end{align*}
$$
Now to find the values of $\beta_0, \beta_1$ and $\sigma^2$ we take the derivative with respect to each and set it equal to 0:

$$ 
\begin{align*}
\frac{\partial l} {\partial {\beta_0}} \bigg\vert_{\hat{\beta_0},\hat{\beta_1},\hat{\sigma}} &= 
\frac{1}{\hat{\sigma}^2}
\sum_{i=1}^n (y_i-\hat{\beta_0}-\hat{\beta}_1x_i) = 0\\
\frac{\partial l}{\partial {\beta_1}} \bigg\vert_{\hat{\beta_0},\hat{\beta_1},\hat{\sigma}} &= \frac{1}{\hat{\sigma}^2} \sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}_1x_i)x_i = 0\\
\frac{\partial l}{\partial {\sigma^2}} \bigg\vert_{\hat{\beta_0},\hat{\beta_1},\hat{\sigma}} &= 
-\frac{1}{2\hat{\sigma}^2} +
\frac{1}{2\hat{\sigma}^4} 
\sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2 = 0\\
\end{align*}
$$

Solving those equations leaves us with the following:

$$
\begin{align*}
\hat{\beta_0} &= 
\overline{y} - \hat{\beta_1} \overline {x}=b_0\\
\hat{\beta_1} &= 
\frac{\sum_{i=1}^n(x_i - \overline{x})y_i}
{\sum_{i=1}^n(x_i -\overline{x})^2} 
= b_1\\
\hat{\sigma^2} &= 
\frac{\sum_{i=1}^n(y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2}
{n} 
\end{align*}
$$

Notes about MLE:

* $\hat{\sigma^2}$ is biased
* MLE requires full distributional assumptions whereas LSE does not
* In general MLE have better statistical properties than LSE