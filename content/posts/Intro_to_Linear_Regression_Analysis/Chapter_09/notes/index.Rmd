---
title: Multicollinearity
author: Jeremy Buss
date: '2021-11-17'
slug: []
categories:
  - Applied Stats
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Regression Analysis
tags:
  - linear regression
draft: yes
katex: yes
---

## Meaning

### Dependency of Regressors:

* In regression, we want to have regressors that are **NOT _moving with each other_**
* Ideally we desire to have **orthogonal** regressors

### What is Multicollinearity?

* After unit length scaling, $\mathbf{X}^{\intercal}\mathbf{X}$ is the correlation matrix of regressors
  - We remove the intercept term or the column of ones in the design matrix, i.e. $\mathbf{X} = [\mathbf{x}_1 \quad \mathbf{x}_2 \quad \dots \quad \mathbf{x}_k]$. The diagnostics will not be exactly the same as when  $\mathbf{X} = [\mathbf{1} \quad \mathbf{x}_1 \quad \mathbf{x}_2 \quad \dots \quad \mathbf{x}_k]$ is used, but the concepts and procedures are the same

For an example let's define two matrices $\mathbf{X}_1$ and $\mathbf{X}_2$:

* Uncorrelated Data:
  - $\mathbf{X}_1^{\intercal}\mathbf{X}_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
  - ($\mathbf{X}_1^{\intercal}\mathbf{X}_1)^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
  - $Var(b_1)= \sigma^2 (\mathbf{X}_1^{\intercal}\mathbf{X}_1)_{11}^{-1}=\sigma^2$
  - $Var(b_2)=\sigma^2$

* Correlated Data:
  - $\mathbf{X}_2^{\intercal}\mathbf{X}_2 = \begin{bmatrix} 1 & 0.992 \\ 0.992 & 1 \end{bmatrix}$
  - ($\mathbf{X}_2^{\intercal}\mathbf{X}_2)^{-1} = \begin{bmatrix} 63.94 & -63.44 \\ -63.44 & 63.94 \end{bmatrix}$
  - $Var(b_1)= \sigma^2 (\mathbf{X}_1^{\intercal}\mathbf{X}_1)_{11}^{-1}=63.94 \sigma^2$
  - $Var(b_2) = 63.94 \sigma^2$

* The variances of the estimates of the correlated regressors are **inflated**, leading to _more uncertainty and less precision_ about coefficient estimation.
* **Multicollinearity** occurs when there are _**near** linear dependencies_ among the $\mathbf{x}_j$s, the columns of $\mathbf{X}$
* _**Near** linear dependencies_: there is a set of constants $c_1, c_2, \dots, c_k$ (not all zero) such that:

$$
\sum_{i=1}^k c_i \mathbf{x}_i \approx \mathbf{0}
$$
  - Example: for $\mathbf{X}_2$ we could have $c_1=1$ and $c_2=-1$ so that
$$
\sum_{i=1}^k c_i \mathbf{x}_i =
1 \times 
\begin{bmatrix}
1\\
0.992
\end{bmatrix} +
-1 \times 
\begin{bmatrix}
0.992\\
1
\end{bmatrix}=
\begin{bmatrix}
0.008\\
-0.008
\end{bmatrix}
$$

## Sources of Multicollinearity

1. Data collection method
2. Constraints on the model or population
3. Model specification
4. A $p > n$ model

### 1. Data Collection

* Multicollinearity occurs when only a subspace of the entire sample has been explored.
* May be able to reduce this multicollinearity through the sampling technique used
  - You can sample additional data in an additional subspace

### 2. Constraints

Physical constraints are present, multicollinearity will exist regardless of collection method. 

For example the relationship between family income and house size.

### 3. Model Specification

* Polynomial terms can cause ill-conditioning in $\mathbf{X}^{\intercal}\mathbf{X}$
* As the order of the model increases, $\mathbf{X}^{\intercal}\mathbf{X}$ matrix inversion will become inaccurate, and error can be introduced into the parameter estimates.
* If range on a regressor variable is small, adding $x^2$ term can result in significant multicollinearity

### 4. $p>n$ Model

* More regressor variables than observations
* The best way to counter this is to remove/reconstruct regressor variables.
  - Principal Component Regression
  - Variable Selection

## Effects

1. **Large variances and covariances** for the LSEs of the regression coefficients.

$$
\mathbf{X}_2^{\intercal}\mathbf{X}_2)^{-1} = \begin{bmatrix} 63.94 & -63.44 \\ -63.44 & 63.94 \end{bmatrix}
$$
2. Tends to produce LSE $b_j$ that are **too large in absolute value**


## Diagnostics

## Solutions