---
title: Model Adequacy Checking
author: Jeremy Buss
date: '2021-10-16'
slug: []
categories:
  - Introduction to Linear Regression Analysis - Montgomery/Peck/Vining
  - Applied Stats
  - Regression Analysis
tags:
  - R
  - linear regression
draft: true
katex: yes
---
```{r echo=FALSE}
library(blogdown)
```

## Assumptions of Linear Regression

Linear Regression:

$$
\begin{align*}
Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \epsilon_i
\end{align*}
$$

![](images/Screenshot 2021-10-16 13.23.21.png)

The major assumptions that we have made concerning Linear Regression are as follows:

1. Relationship between $Y$ and $X_i$ is linear
2. $E(\epsilon_i)=0$
3. $Var(\epsilon_i)=\sigma^2$
4. $Cov(\epsilon_i, \epsilon_j)=0$ for all $i \ne j$
5. $\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)$

* If assumptions are violated, a different sample could lead to different model with opposite conclusions!
* All the inferences are based on the assumption that the model is correct.
* We cannot detect departure from underlying assumptions by examining standard summary statistics. $R^2$, $t$ or $F$ statistics are "global" model properties, and as such they do not ensure model adequacy

## Definition of Residuals

We previously learned that residuals are defined as:

$$
e_i = y_i - \hat{y_i}, \space i=1,2,...,n
$$
However there are a few other constructive ways to view the residuals:

* **deviation** between observed **data** and the **fitted** values
* **unexplained variability** in the response variable from the model
* **realized** or **observed values** of $\epsilon_i$

The residuals have several important properties:

* Zero mean or $\sum_{i=1}^n e_i =0$
* Approximate variance: 

$$
\frac{\sum_{i=1}^n (e_i - \overline{e_i})^2}{n-p} = \frac{\sum_{i=1}^n e_i^2}{n-p} = \frac{SS_{Res}}{n-p} = MS_{Res} = \hat{\sigma}^2
$$
where $p=$ # of parameters

* Unlike $\epsilon_i$, $e_i$ are not independent:
  * $n$ residuals have only $n-p$ degrees of freedom

**Scaled Residuals**: Helpful in finding **outliers**, the **unusual or extreme points** that are considerably different from other data points in $y$ or $x$ directions

## Standardized Residuals

Approximate average variance of a residual is estimated by $MS_{Res}$.

$$
d_i = \frac{e_i}{\sqrt{MS_{Res}}}
$$

* $d_i$s have mean zero and _approximate_ unit variance
* $\vert d_i \vert > 3$ may indicate an outlier

## Variance-Covariance of Residuals

Remember our definition of the Hat Matrix: 

$$
 \mathbf{H} = \mathbf{X} ( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal} 
$$

The Hat Matrix has the following properties:

* $\mathbf{H}$ is symmetric and idempotent
* $\mathbf{I-H}$ is symmetric and idempotent

Also remember that we can show residuals in matrix notation as:

$$
\mathbf{e} = \mathbf{y}-\mathbf{\hat{y}} = \mathbf{y} - \mathbf{X\hat{\beta}}=\mathbf{y}-\mathbf{Hy}=(\mathbf{I}-\mathbf{H})\mathbf{y}
$$
We can take that last equation, properties of the Hat matrix and substituting $\mathbf{y=X\beta+\epsilon}$ into:

$$
\begin{align*}
\mathbf{e} &= (\mathbf{I}-\mathbf{H})(\mathbf{X\beta+\epsilon})\\
&= \mathbf{X\beta}-\mathbf{HX\beta}+(\mathbf{I-H})\mathbf{\epsilon}\\
&= \mathbf{X\beta}-\mathbf{\mathbf{X} ( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal}}\mathbf{X\beta}+(\mathbf{I-H})\mathbf{\epsilon}\\
&= \mathbf{X\beta}-\mathbf{\mathbf{X} [( \mathbf{X^\intercal} \mathbf{X})^{-1} \mathbf{X^\intercal}}\mathbf{X}]\mathbf{\beta}+(\mathbf{I-H})\mathbf{\epsilon}\\
&= \mathbf{X\beta}-\mathbf{X} \mathbf{\beta}+(\mathbf{I-H})\mathbf{\epsilon}\\
&=(\mathbf{I-H})\mathbf{\epsilon}\\
\end{align*}\\
$$

Therefore we see that the residuals are the same linear transformation of the observations $\mathbf{y}$ and the errors $\mathbf{\epsilon}$, or in mathematical terms:

1. $\mathbf{e}=(\mathbf{I} - \mathbf{H})\mathbf{y}$
2. $\mathbf{e}=(\mathbf{I} - \mathbf{H})\mathbf{\epsilon}$

Using the second equation we can find the variance of $\mathbf{e}$

$$
\begin{align*}
Var(\mathbf{e}) &= Var[(\mathbf{I} - \mathbf{H})\mathbf{\epsilon}]\\
\end{align*}
$$
And leveraging the variance of matrices property: 
$$
Var(\mathbf{A}\mathbf{X}) = \mathbf{A}(Var(\mathbf{X}))\mathbf{A^\intercal}
$$ 

Where:

* $\mathbf{X} \in \mathbb{R}^{\ell \times 1}$ is a random column vector 
* $\mathbf{A} \in \mathbb{R}^{k \times \ell}$ is a constant matrix

So back to our variance:

$$
\begin{align*}
Var(\mathbf{e}) &= Var[(\mathbf{I} - \mathbf{H})\mathbf{\epsilon}]\\
&= (\mathbf{I} - \mathbf{H}) Var(\mathbf{\epsilon}) (\mathbf{I} - \mathbf{H}) ^\intercal\\
&= \sigma^2 (\mathbf{I} - \mathbf{H})
\end{align*}
$$

Since the $Var(\mathbf{\epsilon})=\sigma^2 \mathbf{I}$ and $\mathbf{I}-\mathbf{H}$ is symmetric and idempotent. The matrix $\mathbf{I}-\mathbf{H}$ is generally not diagonal, so the residuals have different variances and they are correlated:

* $Var(e_i) = \sigma^2(1-h_{ii})$, where $h_{ii}$ is the $i$-th diagonal element of $\mathbf{H}$
* $Cov(e_i,e_j) = -\sigma^2h_{ij}$, where $h_{ij}$ is the $ij$-th diagonal element of $\mathbf{H}$

* $h_{ii} \in [0,1]$ measures the location of the $i$-th point in $\mathbf{x}$ space

  * the __smaller__ the $h_{ii}$ is, the __closer__ the point $\mathbf{x_i}$ lies to the centroid of the $\mathbf{X}$ space
  * the __larger__ the $h_{ii}$ is, the __smaller__ the $Var(e_i)$ is
  * $MS_{res}>MS_{res}(1-h_{ii})$ overestimates $Var(e_i)$

## Studentized Residuals

$$
r_i = \frac{e_i}{\sqrt{MS_{res}(1-h_{ii})}}
$$

* $r_i$s have mean zero and unit variance, regardless of the location of $\mathbf{x_i}$ when the model form is correct
* Larger than $d_i$
* When $n$ is large, stadardized $d_i$ and studentized $r_i$ are similar


## PRESS Residuals

**PR**ediction **E**ror **S**um of **S**quares **(PRESS):**

$$
\sum_{i=1}^n \left ( y_i - \hat{y}_{(i)} \right )^2
$$
### PRESS residuals:



## Standardized PRESS Residuals

$$
Var[e_{(i)}]=Var \left [ \frac{e_i}{1-h_{ii}} \right ] = \frac{\sigma^2}{1-h_{ii}}
$$

$$
\frac{e_{(i)}}{\sqrt{Var(e_{(i)}})} = \frac{e_{i}/(1-h_{ii})}{\sqrt{\sigma^2/(1 - h_{ii})}} = \frac{e_{i}}{\sqrt{\sigma^2(1 - h_{ii})}}
$$

## R-Student Residuals

* $MS_{res}$ uses all $n$ observations to estimate $\sigma^2$
* $S_{(i)}^2$ estimates $\sigma^2$ based on the data with the $i$-th observation removed:

$$
S_{(i)}^2 = \frac{(n-p)MS_{res}-e_i^2/(1-h_{ii})}{n-p-1}
$$

### R-Student residual:

$$
t_i = \frac{e_i}{\sqrt{S_{(i)}^2(1-h_{ii})}}
$$

* If $i$-th observation is influential, $S_{(i)}^2$ is significantly different from $MS_{res}$
